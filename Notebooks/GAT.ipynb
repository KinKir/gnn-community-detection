{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Attention Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "\n",
    "import performance as pf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, g, in_dim, out_dim):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.g = g\n",
    "        # equation (1)\n",
    "        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        # equation (2)\n",
    "        self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        # edge UDF for equation (2)\n",
    "        z2 = th.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
    "        a = self.attn_fc(z2)\n",
    "        return {'e': F.leaky_relu(a)}\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        # message UDF for equation (3) & (4)\n",
    "        return {'z': edges.src['z'], 'e': edges.data['e']}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        # reduce UDF for equation (3) & (4)\n",
    "        # equation (3)\n",
    "        alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
    "        # equation (4)\n",
    "        h = th.sum(alpha * nodes.mailbox['z'], dim=1)\n",
    "        return {'h': h}\n",
    "\n",
    "    def forward(self, h):\n",
    "        # equation (1)\n",
    "        z = self.fc(h)\n",
    "        self.g.ndata['z'] = z\n",
    "        # equation (2)\n",
    "        self.g.apply_edges(self.edge_attention)\n",
    "        # equation (3) & (4)\n",
    "        self.g.update_all(self.message_func, self.reduce_func)\n",
    "        return self.g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGATLayer(nn.Module):\n",
    "    def __init__(self, g, in_dim, out_dim, num_heads, merge='cat'):\n",
    "        super(MultiHeadGATLayer, self).__init__()\n",
    "        self.heads = nn.ModuleList()\n",
    "        for i in range(num_heads):\n",
    "            self.heads.append(GATLayer(g, in_dim, out_dim))\n",
    "        self.merge = merge\n",
    "\n",
    "    def forward(self, h):\n",
    "        head_outs = [attn_head(h) for attn_head in self.heads]\n",
    "        if self.merge == 'cat':\n",
    "            # concat on the output feature dimension (dim=1)\n",
    "            return th.cat(head_outs, dim=1)\n",
    "        else:\n",
    "            # merge using average\n",
    "            return th.mean(torch.stack(head_outs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, g, in_dim, hidden_dim, out_dim, num_heads):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = MultiHeadGATLayer(g, in_dim, hidden_dim, num_heads)\n",
    "        # Be aware that the input dimension is hidden_dim*num_heads since\n",
    "        # multiple head outputs are concatenated together. Also, only\n",
    "        # one attention head in the output layer.\n",
    "        self.layer2 = MultiHeadGATLayer(g, hidden_dim * num_heads, out_dim, 1)\n",
    "\n",
    "    def forward(self, h):\n",
    "        h = self.layer1(h)\n",
    "        h = F.elu(h)\n",
    "        h = self.layer2(h)\n",
    "        h = F.log_softmax(h, 1)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'GAT' from 'C:\\\\Users\\\\User\\\\Documents\\\\dev\\\\gnn-community-detection\\\\Notebooks\\\\GAT.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(GAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import citation_graph as citegrh\n",
    "import networkx as nx\n",
    "\n",
    "data = citegrh.load_cora()\n",
    "features = th.FloatTensor(data.features)\n",
    "labels = th.LongTensor(data.labels)\n",
    "mask = th.ByteTensor(data.train_mask)\n",
    "g = data.graph\n",
    "\n",
    "# add self loop\n",
    "g.remove_edges_from(nx.selfloop_edges(g))\n",
    "g = DGLGraph(g)\n",
    "g.add_edges(g.nodes(), g.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_train = 0.1\n",
    "percentage_val = 0.1\n",
    "\n",
    "with open(\"../data/permutations/cora_permutation1.pickle\",\"rb\") as f:\n",
    "    perm1 = pickle.load(f)\n",
    "mask_train = np.zeros(g.number_of_nodes())\n",
    "mask_val = np.zeros(g.number_of_nodes())\n",
    "\n",
    "i_train = int(percentage_train*g.number_of_nodes())\n",
    "i_val = i_train + int(percentage_val*g.number_of_nodes())\n",
    "mask_train[perm1[range(0,i_train)]] = 1\n",
    "mask_val[perm1[range(i_train,i_val)]] = 1\n",
    "mask_train = th.BoolTensor(mask_train)\n",
    "mask_val = th.BoolTensor(mask_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = pf.perm_inv_loss(labels)\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6]\n",
      "Epoch 00000 | Loss 1.9453 | Train.Rand 0.0585 | Valid.Rand 0.0462 | Time(s) unknown\n",
      "[2 3 4 6]\n",
      "Epoch 00001 | Loss 1.9422 | Train.Rand 0.1373 | Valid.Rand 0.1070 | Time(s) unknown\n",
      "[2 3 4 6]\n",
      "Epoch 00002 | Loss 1.9403 | Train.Rand 0.1174 | Valid.Rand 0.0776 | Time(s) unknown\n",
      "[3 4 6]\n",
      "Epoch 00003 | Loss 1.9391 | Train.Rand 0.0607 | Valid.Rand 0.0584 | Time(s) 2.1315\n",
      "[3 4 6]\n",
      "Epoch 00004 | Loss 1.9381 | Train.Rand 0.0335 | Valid.Rand 0.0184 | Time(s) 2.1779\n",
      "[3 4]\n",
      "Epoch 00005 | Loss 1.9370 | Train.Rand 0.0138 | Valid.Rand 0.0041 | Time(s) 2.2000\n",
      "[3 4]\n",
      "Epoch 00006 | Loss 1.9356 | Train.Rand 0.0037 | Valid.Rand 0.0031 | Time(s) 2.1635\n",
      "[3 4]\n",
      "Epoch 00007 | Loss 1.9339 | Train.Rand 0.0003 | Valid.Rand 0.0000 | Time(s) 2.1346\n",
      "[3]\n",
      "Epoch 00008 | Loss 1.9317 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.1414\n",
      "[3]\n",
      "Epoch 00009 | Loss 1.9290 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.1492\n",
      "[3]\n",
      "Epoch 00010 | Loss 1.9257 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.1509\n",
      "[3]\n",
      "Epoch 00011 | Loss 1.9219 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.1538\n",
      "[3]\n",
      "Epoch 00012 | Loss 1.9175 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.1517\n",
      "[3]\n",
      "Epoch 00013 | Loss 1.9124 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.1414\n",
      "[3]\n",
      "Epoch 00014 | Loss 1.9066 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.1350\n",
      "[3]\n",
      "Epoch 00015 | Loss 1.9001 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.1285\n",
      "[3]\n",
      "Epoch 00016 | Loss 1.8928 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.1270\n",
      "[3]\n",
      "Epoch 00017 | Loss 1.8846 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.1253\n",
      "[3]\n",
      "Epoch 00018 | Loss 1.8758 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.1212\n",
      "[3]\n",
      "Epoch 00019 | Loss 1.8665 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.1230\n",
      "[3]\n",
      "Epoch 00020 | Loss 1.8569 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.1134\n",
      "[3]\n",
      "Epoch 00021 | Loss 1.8472 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.1060\n",
      "[3]\n",
      "Epoch 00022 | Loss 1.8378 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.1062\n",
      "[3]\n",
      "Epoch 00023 | Loss 1.8288 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.1047\n",
      "[3]\n",
      "Epoch 00024 | Loss 1.8201 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.1032\n",
      "[3]\n",
      "Epoch 00025 | Loss 1.8119 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.1001\n",
      "[3]\n",
      "Epoch 00026 | Loss 1.8042 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.1058\n",
      "[3]\n",
      "Epoch 00027 | Loss 1.7969 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.1067\n",
      "[3]\n",
      "Epoch 00028 | Loss 1.7901 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.1086\n",
      "[3]\n",
      "Epoch 00029 | Loss 1.7838 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.1090\n",
      "[3]\n",
      "Epoch 00030 | Loss 1.7781 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.1054\n",
      "[3]\n",
      "Epoch 00031 | Loss 1.7729 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.0996\n",
      "[3]\n",
      "Epoch 00032 | Loss 1.7683 | Train.Rand 0.0000 | Valid.Rand 0.0000 | Time(s) 2.0975\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-6843db38caae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapproximate_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmask_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[1;31m#loss = F.nll_loss(logits[mask_train], labels[mask_train])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mloss_ev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\dev\\gnn-community-detection\\Notebooks\\performance.py\u001b[0m in \u001b[0;36mapproximate_loss\u001b[1;34m(self, logits, mask, nclasses)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnew_label_perms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             loss = th.min(loss, F.nll_loss(\n\u001b[1;32m---> 52\u001b[1;33m                 new_logits[mask][:, p], new_labels[mask]))\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "net = GAT.GAT_Net(g=g, in_feats=features.shape[1], hidden_size=100, hidden_layers=0, out_feats=len(np.unique(labels)),\n",
    "dropout=0, batchnorm=False, num_heads=1)\n",
    "#print(net)\n",
    "\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=1e-2, weight_decay=1e-2)\n",
    "net.train() # Set to training mode (use dropout)\n",
    "\n",
    "dur = []\n",
    "loss_ev = []\n",
    "current_best = 0 #arbitrarily high\n",
    "current_best_epoch = 0\n",
    "current_best_params = None\n",
    "no_improvement_for = 0\n",
    "\n",
    "for epoch in range(10000):\n",
    "    if epoch >=3:\n",
    "        t0 = time.time()\n",
    "\n",
    "    # Compute loss for test nodes (only for validation, not used by optimizer)\n",
    "    net.eval()\n",
    "    prediction = net(features)\n",
    "    train_rand=pf.rand_score(labels[mask_train].numpy(),np.argmax(prediction[mask_train].detach().numpy(), axis=1))\n",
    "    validation_rand=pf.rand_score(labels[mask_val].numpy(),np.argmax(prediction[mask_val].detach().numpy(), axis=1))\n",
    "    if train_rand>current_best:\n",
    "        current_best = train_rand\n",
    "        current_best_epoch = epoch\n",
    "        current_best_params = copy.deepcopy(net.state_dict())\n",
    "        no_improvement_for = 0\n",
    "    else: no_improvement_for += 1\n",
    "    \n",
    "    if no_improvement_for>100:\n",
    "        break\n",
    "    \n",
    "    net.train()\n",
    "\n",
    "    # Compute loss for train nodes\n",
    "    logits = net(features)\n",
    "\n",
    "    loss = loss_function.approximate_loss(logits,mask_train,nclasses=7)\n",
    "    #loss = F.nll_loss(logits[mask_train], labels[mask_train])\n",
    "    loss_ev.append(loss.detach().item())\n",
    "    print(np.unique(np.argmax(logits[mask_train].detach().numpy(),1)))\n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >=3:\n",
    "        dur.append(time.time() - t0)\n",
    "        print(f\"Epoch {epoch:05d} | Loss {loss.item():.4f} | Train.Rand {train_rand:.4f} | Valid.Rand {validation_rand:.4f} | Time(s) {np.mean(dur):.4f}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch:05d} | Loss {loss.item():.4f} | Train.Rand {train_rand:.4f} | Valid.Rand {validation_rand:.4f} | Time(s) unknown\")\n",
    "        \n",
    "net.load_state_dict(current_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(current_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.argmax(logits[mask_train].detach().numpy(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.argmax(np.exp(prediction.detach()).numpy(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99999994, 1.0000001 , 1.0000001 , ..., 1.        , 0.9999999 ,\n",
       "       0.9999999 ], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.exp(prediction.detach()).numpy(),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 6],\n",
       "       [0, 3],\n",
       "       [2, 2],\n",
       "       [2, 2],\n",
       "       [0, 6],\n",
       "       [6, 4],\n",
       "       [4, 0],\n",
       "       [4, 0],\n",
       "       [0, 2],\n",
       "       [0, 4]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualise predictions\n",
    "net.eval() # Set net to evaluation mode (deactivates dropout)\n",
    "final_prediction = net(g, features).detach()\n",
    "a = np.transpose(np.vstack([final_prediction[mask].numpy().argmax(axis=1),labels[mask].numpy()]))\n",
    "a[a[:,0].argsort()][np.random.choice(range(a.shape[0]),size=10)]\n",
    "# as can be seen, the net predicts other labels, but gets the clusters right :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>All</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Rand-Index</td>\n",
       "      <td>0.085947</td>\n",
       "      <td>0.083860</td>\n",
       "      <td>0.086096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mutual Information</td>\n",
       "      <td>0.245764</td>\n",
       "      <td>0.248938</td>\n",
       "      <td>0.244443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Variation of Information</td>\n",
       "      <td>1.820591</td>\n",
       "      <td>1.701044</td>\n",
       "      <td>1.822282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              All      Train      Test\n",
       "Rand-Index                0.085947  0.083860  0.086096\n",
       "Mutual Information        0.245764  0.248938  0.244443\n",
       "Variation of Information  1.820591  1.701044  1.822282"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval() # Set net to evaluation mode (deactivates dropout)\n",
    "final_prediction = net(features).detach()\n",
    "pf.performance_as_df(labels,final_prediction,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
