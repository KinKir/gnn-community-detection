{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Convolutional Network by Kipf and Welling"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GNN Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModule(nn.Module):\n",
    "    \"\"\"The linear transformation part of the GCN layer\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(LinearModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation # This is the activation function\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        h = self.activation(h)\n",
    "        return {'h' : h}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\"A GCN layer\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = LinearModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(message_func=fn.copy_src(src='h', out='m'), reduce_func=fn.sum(msg='m', out='h'))\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.gcn1 = GCN(1433, 21, F.relu)\n",
    "        self.gcn2 = GCN(21, 7, lambda x: F.log_softmax(x,1))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = self.gcn1(g, features)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.gcn2(g, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import citation_graph as citegrh\n",
    "import networkx as nx\n",
    "\n",
    "data = citegrh.load_cora()\n",
    "features = th.FloatTensor(data.features)\n",
    "labels = th.LongTensor(data.labels)\n",
    "mask = th.ByteTensor(data.train_mask)\n",
    "g = data.graph\n",
    "\n",
    "# add self loop\n",
    "g.remove_edges_from(nx.selfloop_edges(g))\n",
    "g = DGLGraph(g)\n",
    "g.add_edges(g.nodes(), g.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_train = 0.02\n",
    "\n",
    "with open(\"data/cora_permutation1.pickle\",\"rb\") as f:\n",
    "    perm1 = pickle.load(f)\n",
    "mask = np.zeros(g.number_of_nodes())\n",
    "mask[perm1[range(int(percentage_train*g.number_of_nodes()))]] = 1\n",
    "mask = th.ByteTensor(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Users\\a_liso02\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n  out=out, **kwargs)\nC:\\Users\\a_liso02\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\nEpoch 00000 | Loss 1.9675 | Val.Loss 1.9583 | Time(s) nan\nEpoch 00001 | Loss 1.9286 | Val.Loss 1.9420 | Time(s) nan\nEpoch 00002 | Loss 1.8919 | Val.Loss 1.9260 | Time(s) nan\nEpoch 00003 | Loss 1.8561 | Val.Loss 1.9105 | Time(s) 0.1420\nEpoch 00004 | Loss 1.8184 | Val.Loss 1.8935 | Time(s) 0.1425\nEpoch 00005 | Loss 1.7803 | Val.Loss 1.8764 | Time(s) 0.1407\nEpoch 00006 | Loss 1.7423 | Val.Loss 1.8591 | Time(s) 0.1398\nEpoch 00007 | Loss 1.7073 | Val.Loss 1.8428 | Time(s) 0.1388\nEpoch 00008 | Loss 1.6744 | Val.Loss 1.8277 | Time(s) 0.1390\nEpoch 00009 | Loss 1.6444 | Val.Loss 1.8137 | Time(s) 0.1386\nEpoch 00010 | Loss 1.6165 | Val.Loss 1.8005 | Time(s) 0.1385\nEpoch 00011 | Loss 1.5898 | Val.Loss 1.7878 | Time(s) 0.1391\nEpoch 00012 | Loss 1.5643 | Val.Loss 1.7753 | Time(s) 0.1432\nEpoch 00013 | Loss 1.5392 | Val.Loss 1.7630 | Time(s) 0.1427\nEpoch 00014 | Loss 1.5154 | Val.Loss 1.7509 | Time(s) 0.1428\nEpoch 00015 | Loss 1.4924 | Val.Loss 1.7387 | Time(s) 0.1429\nEpoch 00016 | Loss 1.4702 | Val.Loss 1.7266 | Time(s) 0.1430\nEpoch 00017 | Loss 1.4487 | Val.Loss 1.7145 | Time(s) 0.1439\nEpoch 00018 | Loss 1.4278 | Val.Loss 1.7024 | Time(s) 0.1462\nEpoch 00019 | Loss 1.4076 | Val.Loss 1.6905 | Time(s) 0.1462\nEpoch 00020 | Loss 1.3881 | Val.Loss 1.6788 | Time(s) 0.1466\nEpoch 00021 | Loss 1.3692 | Val.Loss 1.6672 | Time(s) 0.1466\nEpoch 00022 | Loss 1.3508 | Val.Loss 1.6559 | Time(s) 0.1471\nEpoch 00023 | Loss 1.3329 | Val.Loss 1.6450 | Time(s) 0.1470\nEpoch 00024 | Loss 1.3154 | Val.Loss 1.6343 | Time(s) 0.1473\nEpoch 00025 | Loss 1.2983 | Val.Loss 1.6240 | Time(s) 0.1475\nEpoch 00026 | Loss 1.2815 | Val.Loss 1.6141 | Time(s) 0.1475\nEpoch 00027 | Loss 1.2650 | Val.Loss 1.6046 | Time(s) 0.1475\nEpoch 00028 | Loss 1.2488 | Val.Loss 1.5955 | Time(s) 0.1481\nEpoch 00029 | Loss 1.2328 | Val.Loss 1.5867 | Time(s) 0.1483\nEpoch 00030 | Loss 1.2169 | Val.Loss 1.5782 | Time(s) 0.1485\nEpoch 00031 | Loss 1.2011 | Val.Loss 1.5699 | Time(s) 0.1485\nEpoch 00032 | Loss 1.1856 | Val.Loss 1.5618 | Time(s) 0.1486\nEpoch 00033 | Loss 1.1701 | Val.Loss 1.5539 | Time(s) 0.1484\nEpoch 00034 | Loss 1.1548 | Val.Loss 1.5461 | Time(s) 0.1483\nEpoch 00035 | Loss 1.1397 | Val.Loss 1.5384 | Time(s) 0.1488\nEpoch 00036 | Loss 1.1247 | Val.Loss 1.5308 | Time(s) 0.1489\nEpoch 00037 | Loss 1.1099 | Val.Loss 1.5235 | Time(s) 0.1489\nEpoch 00038 | Loss 1.0952 | Val.Loss 1.5164 | Time(s) 0.1488\nEpoch 00039 | Loss 1.0805 | Val.Loss 1.5094 | Time(s) 0.1487\nEpoch 00040 | Loss 1.0660 | Val.Loss 1.5026 | Time(s) 0.1490\nEpoch 00041 | Loss 1.0516 | Val.Loss 1.4959 | Time(s) 0.1493\nEpoch 00042 | Loss 1.0373 | Val.Loss 1.4893 | Time(s) 0.1496\nEpoch 00043 | Loss 1.0232 | Val.Loss 1.4829 | Time(s) 0.1495\nEpoch 00044 | Loss 1.0092 | Val.Loss 1.4765 | Time(s) 0.1495\nEpoch 00045 | Loss 0.9953 | Val.Loss 1.4703 | Time(s) 0.1498\nEpoch 00046 | Loss 0.9815 | Val.Loss 1.4641 | Time(s) 0.1521\nEpoch 00047 | Loss 0.9679 | Val.Loss 1.4581 | Time(s) 0.1528\nEpoch 00048 | Loss 0.9545 | Val.Loss 1.4521 | Time(s) 0.1532\nEpoch 00049 | Loss 0.9411 | Val.Loss 1.4463 | Time(s) 0.1529\nEpoch 00050 | Loss 0.9278 | Val.Loss 1.4404 | Time(s) 0.1527\nEpoch 00051 | Loss 0.9147 | Val.Loss 1.4347 | Time(s) 0.1525\nEpoch 00052 | Loss 0.9016 | Val.Loss 1.4290 | Time(s) 0.1525\nEpoch 00053 | Loss 0.8887 | Val.Loss 1.4234 | Time(s) 0.1527\nEpoch 00054 | Loss 0.8758 | Val.Loss 1.4178 | Time(s) 0.1528\nEpoch 00055 | Loss 0.8631 | Val.Loss 1.4124 | Time(s) 0.1527\nEpoch 00056 | Loss 0.8505 | Val.Loss 1.4070 | Time(s) 0.1526\nEpoch 00057 | Loss 0.8380 | Val.Loss 1.4017 | Time(s) 0.1525\nEpoch 00058 | Loss 0.8256 | Val.Loss 1.3964 | Time(s) 0.1525\nEpoch 00059 | Loss 0.8133 | Val.Loss 1.3911 | Time(s) 0.1523\nEpoch 00060 | Loss 0.8011 | Val.Loss 1.3859 | Time(s) 0.1523\nEpoch 00061 | Loss 0.7891 | Val.Loss 1.3808 | Time(s) 0.1523\nEpoch 00062 | Loss 0.7771 | Val.Loss 1.3758 | Time(s) 0.1521\nEpoch 00063 | Loss 0.7653 | Val.Loss 1.3708 | Time(s) 0.1521\nEpoch 00064 | Loss 0.7536 | Val.Loss 1.3660 | Time(s) 0.1519\nEpoch 00065 | Loss 0.7421 | Val.Loss 1.3613 | Time(s) 0.1517\nEpoch 00066 | Loss 0.7306 | Val.Loss 1.3567 | Time(s) 0.1519\nEpoch 00067 | Loss 0.7192 | Val.Loss 1.3522 | Time(s) 0.1518\nEpoch 00068 | Loss 0.7080 | Val.Loss 1.3479 | Time(s) 0.1518\nEpoch 00069 | Loss 0.6970 | Val.Loss 1.3437 | Time(s) 0.1517\nEpoch 00070 | Loss 0.6860 | Val.Loss 1.3396 | Time(s) 0.1518\nEpoch 00071 | Loss 0.6751 | Val.Loss 1.3357 | Time(s) 0.1517\nEpoch 00072 | Loss 0.6644 | Val.Loss 1.3318 | Time(s) 0.1516\nEpoch 00073 | Loss 0.6538 | Val.Loss 1.3281 | Time(s) 0.1515\nEpoch 00074 | Loss 0.6434 | Val.Loss 1.3244 | Time(s) 0.1520\nEpoch 00075 | Loss 0.6331 | Val.Loss 1.3208 | Time(s) 0.1518\nEpoch 00076 | Loss 0.6229 | Val.Loss 1.3174 | Time(s) 0.1518\nEpoch 00077 | Loss 0.6128 | Val.Loss 1.3140 | Time(s) 0.1517\nEpoch 00078 | Loss 0.6028 | Val.Loss 1.3108 | Time(s) 0.1519\nEpoch 00079 | Loss 0.5930 | Val.Loss 1.3076 | Time(s) 0.1518\nEpoch 00080 | Loss 0.5833 | Val.Loss 1.3045 | Time(s) 0.1517\nEpoch 00081 | Loss 0.5737 | Val.Loss 1.3016 | Time(s) 0.1517\nEpoch 00082 | Loss 0.5643 | Val.Loss 1.2987 | Time(s) 0.1516\nEpoch 00083 | Loss 0.5550 | Val.Loss 1.2960 | Time(s) 0.1516\nEpoch 00084 | Loss 0.5458 | Val.Loss 1.2934 | Time(s) 0.1518\nEpoch 00085 | Loss 0.5367 | Val.Loss 1.2910 | Time(s) 0.1517\nEpoch 00086 | Loss 0.5278 | Val.Loss 1.2886 | Time(s) 0.1517\nEpoch 00087 | Loss 0.5190 | Val.Loss 1.2863 | Time(s) 0.1518\nEpoch 00088 | Loss 0.5103 | Val.Loss 1.2841 | Time(s) 0.1517\nEpoch 00089 | Loss 0.5017 | Val.Loss 1.2821 | Time(s) 0.1517\nEpoch 00090 | Loss 0.4933 | Val.Loss 1.2801 | Time(s) 0.1519\nEpoch 00091 | Loss 0.4849 | Val.Loss 1.2782 | Time(s) 0.1519\nEpoch 00092 | Loss 0.4767 | Val.Loss 1.2764 | Time(s) 0.1520\nEpoch 00093 | Loss 0.4687 | Val.Loss 1.2747 | Time(s) 0.1519\nEpoch 00094 | Loss 0.4607 | Val.Loss 1.2730 | Time(s) 0.1518\nEpoch 00095 | Loss 0.4529 | Val.Loss 1.2714 | Time(s) 0.1522\nEpoch 00096 | Loss 0.4452 | Val.Loss 1.2699 | Time(s) 0.1522\nEpoch 00097 | Loss 0.4376 | Val.Loss 1.2684 | Time(s) 0.1522\nEpoch 00098 | Loss 0.4301 | Val.Loss 1.2670 | Time(s) 0.1520\nEpoch 00099 | Loss 0.4228 | Val.Loss 1.2657 | Time(s) 0.1522\nEpoch 00100 | Loss 0.4155 | Val.Loss 1.2645 | Time(s) 0.1522\nEpoch 00101 | Loss 0.4085 | Val.Loss 1.2633 | Time(s) 0.1521\nEpoch 00102 | Loss 0.4015 | Val.Loss 1.2622 | Time(s) 0.1519\nEpoch 00103 | Loss 0.3946 | Val.Loss 1.2611 | Time(s) 0.1521\nEpoch 00104 | Loss 0.3879 | Val.Loss 1.2601 | Time(s) 0.1520\nEpoch 00105 | Loss 0.3813 | Val.Loss 1.2592 | Time(s) 0.1520\nEpoch 00106 | Loss 0.3747 | Val.Loss 1.2584 | Time(s) 0.1520\nEpoch 00107 | Loss 0.3683 | Val.Loss 1.2575 | Time(s) 0.1522\nEpoch 00108 | Loss 0.3620 | Val.Loss 1.2567 | Time(s) 0.1521\nEpoch 00109 | Loss 0.3558 | Val.Loss 1.2559 | Time(s) 0.1522\nEpoch 00110 | Loss 0.3497 | Val.Loss 1.2552 | Time(s) 0.1522\nEpoch 00111 | Loss 0.3438 | Val.Loss 1.2546 | Time(s) 0.1522\nEpoch 00112 | Loss 0.3379 | Val.Loss 1.2541 | Time(s) 0.1521\nEpoch 00113 | Loss 0.3321 | Val.Loss 1.2536 | Time(s) 0.1522\nEpoch 00114 | Loss 0.3265 | Val.Loss 1.2532 | Time(s) 0.1521\nEpoch 00115 | Loss 0.3209 | Val.Loss 1.2529 | Time(s) 0.1521\nEpoch 00116 | Loss 0.3155 | Val.Loss 1.2526 | Time(s) 0.1521\nEpoch 00117 | Loss 0.3101 | Val.Loss 1.2524 | Time(s) 0.1522\nEpoch 00118 | Loss 0.3049 | Val.Loss 1.2523 | Time(s) 0.1522\nEpoch 00119 | Loss 0.2997 | Val.Loss 1.2523 | Time(s) 0.1523\nEpoch 00120 | Loss 0.2946 | Val.Loss 1.2522 | Time(s) 0.1522\nEpoch 00121 | Loss 0.2897 | Val.Loss 1.2522 | Time(s) 0.1522\nEpoch 00122 | Loss 0.2848 | Val.Loss 1.2523 | Time(s) 0.1524\nEpoch 00123 | Loss 0.2800 | Val.Loss 1.2524 | Time(s) 0.1524\nEpoch 00124 | Loss 0.2753 | Val.Loss 1.2525 | Time(s) 0.1525\nEpoch 00125 | Loss 0.2707 | Val.Loss 1.2526 | Time(s) 0.1524\nEpoch 00126 | Loss 0.2662 | Val.Loss 1.2527 | Time(s) 0.1523\nEpoch 00127 | Loss 0.2618 | Val.Loss 1.2529 | Time(s) 0.1522\nEpoch 00128 | Loss 0.2574 | Val.Loss 1.2532 | Time(s) 0.1523\nEpoch 00129 | Loss 0.2532 | Val.Loss 1.2535 | Time(s) 0.1523\nEpoch 00130 | Loss 0.2490 | Val.Loss 1.2538 | Time(s) 0.1523\nEpoch 00131 | Loss 0.2449 | Val.Loss 1.2541 | Time(s) 0.1522\nEpoch 00132 | Loss 0.2409 | Val.Loss 1.2543 | Time(s) 0.1523\nEpoch 00133 | Loss 0.2370 | Val.Loss 1.2545 | Time(s) 0.1522\nEpoch 00134 | Loss 0.2331 | Val.Loss 1.2548 | Time(s) 0.1522\nEpoch 00135 | Loss 0.2293 | Val.Loss 1.2551 | Time(s) 0.1521\nEpoch 00136 | Loss 0.2256 | Val.Loss 1.2555 | Time(s) 0.1522\nEpoch 00137 | Loss 0.2220 | Val.Loss 1.2559 | Time(s) 0.1525\nEpoch 00138 | Loss 0.2185 | Val.Loss 1.2563 | Time(s) 0.1526\nEpoch 00139 | Loss 0.2150 | Val.Loss 1.2568 | Time(s) 0.1525\nEpoch 00140 | Loss 0.2116 | Val.Loss 1.2574 | Time(s) 0.1525\nEpoch 00141 | Loss 0.2082 | Val.Loss 1.2580 | Time(s) 0.1524\nEpoch 00142 | Loss 0.2050 | Val.Loss 1.2587 | Time(s) 0.1524\nEpoch 00143 | Loss 0.2018 | Val.Loss 1.2594 | Time(s) 0.1524\nEpoch 00144 | Loss 0.1986 | Val.Loss 1.2600 | Time(s) 0.1524\nEpoch 00145 | Loss 0.1955 | Val.Loss 1.2607 | Time(s) 0.1524\nEpoch 00146 | Loss 0.1925 | Val.Loss 1.2614 | Time(s) 0.1524\nEpoch 00147 | Loss 0.1895 | Val.Loss 1.2621 | Time(s) 0.1525\nEpoch 00148 | Loss 0.1866 | Val.Loss 1.2629 | Time(s) 0.1525\nEpoch 00149 | Loss 0.1838 | Val.Loss 1.2637 | Time(s) 0.1524\n"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "net = Net()\n",
    "#print(net)\n",
    "\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=1e-3)\n",
    "net.train() # Set to training mode (use dropout)\n",
    "\n",
    "dur = []\n",
    "for epoch in range(150):\n",
    "    if epoch >=3:\n",
    "        t0 = time.time()\n",
    "\n",
    "    # Compute loss for test nodes (only for validation, not used by optimizer)\n",
    "    net.eval()\n",
    "    prediction = net(g, features)\n",
    "    val_loss = F.nll_loss(prediction.detach()[1-mask], labels[1-mask])\n",
    "    net.train()\n",
    "\n",
    "    # Compute loss for train nodes\n",
    "    logits = net(g, features)\n",
    "    loss = F.nll_loss(logits[mask], labels[mask])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >=3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Val.Loss {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), val_loss.item(), np.mean(dur)))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nNLL-Loss:\n All : 1.2504 | Train: 0.1874 | Test: 1.2720 |\n\nAccuracy:\n All : 0.5934 | Train: 1.0000 | Test: 0.5852 |\n"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score as acc\n",
    "net.eval() # Set net to evaluation mode (deactivates dropout)\n",
    "final_prediction = net(g, features).detach()\n",
    "\n",
    "pred_sets = {\"All \":final_prediction,\"Train\":final_prediction[mask],\"Test\":final_prediction[1-mask]}\n",
    "label_sets = {\"All \":labels,\"Train\":labels[mask],\"Test\":labels[1-mask]}\n",
    "eval_functions = {\"NLL-Loss\":lambda y,x: F.nll_loss(x,y),\"Accuracy\":lambda y,x: acc(y,x.numpy().argmax(axis=1))}\n",
    "\n",
    "for name,func in eval_functions.items():\n",
    "    eval_message = f\"\\n{name}:\\n\"\n",
    "    for subset in pred_sets.keys():\n",
    "        eval_message += f\" {subset}: {func(label_sets[subset],pred_sets[subset]):.4f} |\"\n",
    "    print(eval_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}