{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Convolutional Network by Kipf and Welling"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "\n",
    "import Notebooks.performance as pf"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GNN Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModule(nn.Module):\n",
    "    \"\"\"The linear transformation part of the GCN layer\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(LinearModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation # This is the activation function\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        h = self.activation(h)\n",
    "        return {'h' : h}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\"A GCN layer\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = LinearModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(message_func=fn.copy_src(src='h', out='m'), reduce_func=fn.sum(msg='m', out='h'))\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, infeats, hidden_size, outfeats):\n",
    "        super(Net, self).__init__()\n",
    "        self.gcn1 = GCN(infeats, hidden_size, F.relu)\n",
    "        self.gcn2 = GCN(hidden_size, outfeats, lambda x: F.log_softmax(x,1))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = self.gcn1(g, features)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.gcn2(g, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import citation_graph as citegrh\n",
    "import networkx as nx\n",
    "\n",
    "data = citegrh.load_cora()\n",
    "features = th.FloatTensor(data.features)\n",
    "labels = th.LongTensor(data.labels)\n",
    "mask = th.ByteTensor(data.train_mask)\n",
    "g = data.graph\n",
    "\n",
    "# add self loop\n",
    "g.remove_edges_from(nx.selfloop_edges(g))\n",
    "g = DGLGraph(g)\n",
    "g.add_edges(g.nodes(), g.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_train = 0.02\n",
    "\n",
    "with open(\"data/cora_permutation1.pickle\",\"rb\") as f:\n",
    "    perm1 = pickle.load(f)\n",
    "mask = np.zeros(g.number_of_nodes())\n",
    "mask[perm1[range(int(percentage_train*g.number_of_nodes()))]] = 1\n",
    "mask = th.ByteTensor(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Users\\a_liso02\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n  out=out, **kwargs)\nC:\\Users\\a_liso02\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\nEpoch 00000 | Loss 1.8940 | Val.Loss nan | Time(s) nan\nEpoch 00001 | Loss 1.8565 | Val.Loss nan | Time(s) nan\nEpoch 00002 | Loss 1.8246 | Val.Loss nan | Time(s) nan\nEpoch 00003 | Loss 1.7983 | Val.Loss nan | Time(s) 1.0251\nEpoch 00004 | Loss 1.7706 | Val.Loss nan | Time(s) 1.0251\nEpoch 00005 | Loss 1.7418 | Val.Loss nan | Time(s) 1.0294\nEpoch 00006 | Loss 1.7126 | Val.Loss nan | Time(s) 1.0236\nEpoch 00007 | Loss 1.6834 | Val.Loss nan | Time(s) 1.0273\nEpoch 00008 | Loss 1.6549 | Val.Loss nan | Time(s) 1.0296\nEpoch 00009 | Loss 1.6268 | Val.Loss nan | Time(s) 1.0278\nEpoch 00010 | Loss 1.5995 | Val.Loss nan | Time(s) 1.0396\nEpoch 00011 | Loss 1.5733 | Val.Loss nan | Time(s) 1.0403\nEpoch 00012 | Loss 1.5485 | Val.Loss nan | Time(s) 1.0406\nEpoch 00013 | Loss 1.5245 | Val.Loss nan | Time(s) 1.0384\nEpoch 00014 | Loss 1.5023 | Val.Loss nan | Time(s) 1.0413\nEpoch 00015 | Loss 1.4808 | Val.Loss nan | Time(s) 1.0431\nEpoch 00016 | Loss 1.4598 | Val.Loss nan | Time(s) 1.0432\nEpoch 00017 | Loss 1.4395 | Val.Loss nan | Time(s) 1.0400\nEpoch 00018 | Loss 1.4199 | Val.Loss nan | Time(s) 1.0378\nEpoch 00019 | Loss 1.4014 | Val.Loss nan | Time(s) 1.0360\nEpoch 00020 | Loss 1.3834 | Val.Loss nan | Time(s) 1.0343\nEpoch 00021 | Loss 1.3659 | Val.Loss nan | Time(s) 1.0320\nEpoch 00022 | Loss 1.3489 | Val.Loss nan | Time(s) 1.0302\nEpoch 00023 | Loss 1.3321 | Val.Loss nan | Time(s) 1.0303\nEpoch 00024 | Loss 1.3156 | Val.Loss nan | Time(s) 1.0290\nEpoch 00025 | Loss 1.2992 | Val.Loss nan | Time(s) 1.0314\nEpoch 00026 | Loss 1.2830 | Val.Loss nan | Time(s) 1.0354\nEpoch 00027 | Loss 1.2668 | Val.Loss nan | Time(s) 1.0379\nEpoch 00028 | Loss 1.2507 | Val.Loss nan | Time(s) 1.0383\nEpoch 00029 | Loss 1.2348 | Val.Loss nan | Time(s) 1.0377\nEpoch 00030 | Loss 1.2192 | Val.Loss nan | Time(s) 1.0465\nEpoch 00031 | Loss 1.2038 | Val.Loss nan | Time(s) 1.0526\nEpoch 00032 | Loss 1.1886 | Val.Loss nan | Time(s) 1.0561\nEpoch 00033 | Loss 1.1736 | Val.Loss nan | Time(s) 1.0613\nEpoch 00034 | Loss 1.1589 | Val.Loss nan | Time(s) 1.0627\nEpoch 00035 | Loss 1.1443 | Val.Loss nan | Time(s) 1.0645\nEpoch 00036 | Loss 1.1299 | Val.Loss nan | Time(s) 1.0685\nEpoch 00037 | Loss 1.1157 | Val.Loss nan | Time(s) 1.0744\nEpoch 00038 | Loss 1.1016 | Val.Loss nan | Time(s) 1.0761\nEpoch 00039 | Loss 1.0878 | Val.Loss nan | Time(s) 1.0778\nEpoch 00040 | Loss 1.0741 | Val.Loss nan | Time(s) 1.0761\nEpoch 00041 | Loss 1.0606 | Val.Loss nan | Time(s) 1.0752\nEpoch 00042 | Loss 1.0472 | Val.Loss nan | Time(s) 1.0750\nEpoch 00043 | Loss 1.0339 | Val.Loss nan | Time(s) 1.0745\nEpoch 00044 | Loss 1.0205 | Val.Loss nan | Time(s) 1.0736\nEpoch 00045 | Loss 1.0071 | Val.Loss nan | Time(s) 1.0735\nEpoch 00046 | Loss 0.9937 | Val.Loss nan | Time(s) 1.0723\nEpoch 00047 | Loss 0.9803 | Val.Loss nan | Time(s) 1.0736\nEpoch 00048 | Loss 0.9670 | Val.Loss nan | Time(s) 1.0726\nEpoch 00049 | Loss 0.9538 | Val.Loss nan | Time(s) 1.0734\nEpoch 00050 | Loss 0.9408 | Val.Loss nan | Time(s) 1.0723\nEpoch 00051 | Loss 0.9279 | Val.Loss nan | Time(s) 1.0752\nEpoch 00052 | Loss 0.9151 | Val.Loss nan | Time(s) 1.0773\nEpoch 00053 | Loss 0.9023 | Val.Loss nan | Time(s) 1.0803\nEpoch 00054 | Loss 0.8896 | Val.Loss nan | Time(s) 1.0809\nEpoch 00055 | Loss 0.8769 | Val.Loss nan | Time(s) 1.0836\nEpoch 00056 | Loss 0.8643 | Val.Loss nan | Time(s) 1.0859\nEpoch 00057 | Loss 0.8517 | Val.Loss nan | Time(s) 1.0889\nEpoch 00058 | Loss 0.8393 | Val.Loss nan | Time(s) 1.0932\nEpoch 00059 | Loss 0.8269 | Val.Loss nan | Time(s) 1.0930\nEpoch 00060 | Loss 0.8147 | Val.Loss nan | Time(s) 1.0985\nEpoch 00061 | Loss 0.8025 | Val.Loss nan | Time(s) 1.1027\nEpoch 00062 | Loss 0.7904 | Val.Loss nan | Time(s) 1.1061\nEpoch 00063 | Loss 0.7784 | Val.Loss nan | Time(s) 1.1081\nEpoch 00064 | Loss 0.7665 | Val.Loss nan | Time(s) 1.1090\nEpoch 00065 | Loss 0.7547 | Val.Loss nan | Time(s) 1.1100\nEpoch 00066 | Loss 0.7430 | Val.Loss nan | Time(s) 1.1135\nEpoch 00067 | Loss 0.7313 | Val.Loss nan | Time(s) 1.1169\nEpoch 00068 | Loss 0.7197 | Val.Loss nan | Time(s) 1.1186\nEpoch 00069 | Loss 0.7081 | Val.Loss nan | Time(s) 1.1180\nEpoch 00070 | Loss 0.6966 | Val.Loss nan | Time(s) 1.1171\nEpoch 00071 | Loss 0.6853 | Val.Loss nan | Time(s) 1.1175\nEpoch 00072 | Loss 0.6740 | Val.Loss nan | Time(s) 1.1172\nEpoch 00073 | Loss 0.6628 | Val.Loss nan | Time(s) 1.1215\nEpoch 00074 | Loss 0.6518 | Val.Loss nan | Time(s) 1.1252\nEpoch 00075 | Loss 0.6408 | Val.Loss nan | Time(s) 1.1334\nEpoch 00076 | Loss 0.6299 | Val.Loss nan | Time(s) 1.1342\nEpoch 00077 | Loss 0.6191 | Val.Loss nan | Time(s) 1.1333\nEpoch 00078 | Loss 0.6085 | Val.Loss nan | Time(s) 1.1329\nEpoch 00079 | Loss 0.5979 | Val.Loss nan | Time(s) 1.1343\nEpoch 00080 | Loss 0.5875 | Val.Loss nan | Time(s) 1.1359\nEpoch 00081 | Loss 0.5771 | Val.Loss nan | Time(s) 1.1387\nEpoch 00082 | Loss 0.5669 | Val.Loss nan | Time(s) 1.1423\nEpoch 00083 | Loss 0.5568 | Val.Loss nan | Time(s) 1.1449\nEpoch 00084 | Loss 0.5469 | Val.Loss nan | Time(s) 1.1482\nEpoch 00085 | Loss 0.5371 | Val.Loss nan | Time(s) 1.1530\nEpoch 00086 | Loss 0.5274 | Val.Loss nan | Time(s) 1.1548\nEpoch 00087 | Loss 0.5178 | Val.Loss nan | Time(s) 1.1557\nEpoch 00088 | Loss 0.5084 | Val.Loss nan | Time(s) 1.1547\nEpoch 00089 | Loss 0.4991 | Val.Loss nan | Time(s) 1.1531\nEpoch 00090 | Loss 0.4900 | Val.Loss nan | Time(s) 1.1515\nEpoch 00091 | Loss 0.4810 | Val.Loss nan | Time(s) 1.1498\nEpoch 00092 | Loss 0.4722 | Val.Loss nan | Time(s) 1.1488\nEpoch 00093 | Loss 0.4635 | Val.Loss nan | Time(s) 1.1475\nEpoch 00094 | Loss 0.4549 | Val.Loss nan | Time(s) 1.1461\nEpoch 00095 | Loss 0.4465 | Val.Loss nan | Time(s) 1.1445\nEpoch 00096 | Loss 0.4382 | Val.Loss nan | Time(s) 1.1435\nEpoch 00097 | Loss 0.4301 | Val.Loss nan | Time(s) 1.1424\nEpoch 00098 | Loss 0.4221 | Val.Loss nan | Time(s) 1.1417\nEpoch 00099 | Loss 0.4143 | Val.Loss nan | Time(s) 1.1408\nEpoch 00100 | Loss 0.4066 | Val.Loss nan | Time(s) 1.1397\nEpoch 00101 | Loss 0.3990 | Val.Loss nan | Time(s) 1.1389\nEpoch 00102 | Loss 0.3916 | Val.Loss nan | Time(s) 1.1382\nEpoch 00103 | Loss 0.3843 | Val.Loss nan | Time(s) 1.1372\nEpoch 00104 | Loss 0.3772 | Val.Loss nan | Time(s) 1.1366\nEpoch 00105 | Loss 0.3702 | Val.Loss nan | Time(s) 1.1359\nEpoch 00106 | Loss 0.3633 | Val.Loss nan | Time(s) 1.1350\nEpoch 00107 | Loss 0.3565 | Val.Loss nan | Time(s) 1.1342\nEpoch 00108 | Loss 0.3499 | Val.Loss nan | Time(s) 1.1336\nEpoch 00109 | Loss 0.3434 | Val.Loss nan | Time(s) 1.1327\nEpoch 00110 | Loss 0.3371 | Val.Loss nan | Time(s) 1.1323\nEpoch 00111 | Loss 0.3308 | Val.Loss nan | Time(s) 1.1315\nEpoch 00112 | Loss 0.3247 | Val.Loss nan | Time(s) 1.1308\nEpoch 00113 | Loss 0.3187 | Val.Loss nan | Time(s) 1.1300\nEpoch 00114 | Loss 0.3129 | Val.Loss nan | Time(s) 1.1292\nEpoch 00115 | Loss 0.3071 | Val.Loss nan | Time(s) 1.1287\nEpoch 00116 | Loss 0.3015 | Val.Loss nan | Time(s) 1.1285\nEpoch 00117 | Loss 0.2960 | Val.Loss nan | Time(s) 1.1283\nEpoch 00118 | Loss 0.2906 | Val.Loss nan | Time(s) 1.1279\nEpoch 00119 | Loss 0.2854 | Val.Loss nan | Time(s) 1.1271\nEpoch 00120 | Loss 0.2802 | Val.Loss nan | Time(s) 1.1261\nEpoch 00121 | Loss 0.2751 | Val.Loss nan | Time(s) 1.1253\nEpoch 00122 | Loss 0.2702 | Val.Loss nan | Time(s) 1.1262\nEpoch 00123 | Loss 0.2654 | Val.Loss nan | Time(s) 1.1256\nEpoch 00124 | Loss 0.2606 | Val.Loss nan | Time(s) 1.1255\nEpoch 00125 | Loss 0.2560 | Val.Loss nan | Time(s) 1.1246\nEpoch 00126 | Loss 0.2514 | Val.Loss nan | Time(s) 1.1237\nEpoch 00127 | Loss 0.2470 | Val.Loss nan | Time(s) 1.1229\nEpoch 00128 | Loss 0.2426 | Val.Loss nan | Time(s) 1.1225\nEpoch 00129 | Loss 0.2384 | Val.Loss nan | Time(s) 1.1217\nEpoch 00130 | Loss 0.2342 | Val.Loss nan | Time(s) 1.1209\nEpoch 00131 | Loss 0.2301 | Val.Loss nan | Time(s) 1.1202\nEpoch 00132 | Loss 0.2261 | Val.Loss nan | Time(s) 1.1196\nEpoch 00133 | Loss 0.2222 | Val.Loss nan | Time(s) 1.1193\nEpoch 00134 | Loss 0.2184 | Val.Loss nan | Time(s) 1.1189\nEpoch 00135 | Loss 0.2147 | Val.Loss nan | Time(s) 1.1183\nEpoch 00136 | Loss 0.2110 | Val.Loss nan | Time(s) 1.1174\nEpoch 00137 | Loss 0.2074 | Val.Loss nan | Time(s) 1.1168\nEpoch 00138 | Loss 0.2039 | Val.Loss nan | Time(s) 1.1161\nEpoch 00139 | Loss 0.2005 | Val.Loss nan | Time(s) 1.1156\nEpoch 00140 | Loss 0.1971 | Val.Loss nan | Time(s) 1.1148\nEpoch 00141 | Loss 0.1939 | Val.Loss nan | Time(s) 1.1144\nEpoch 00142 | Loss 0.1906 | Val.Loss nan | Time(s) 1.1137\nEpoch 00143 | Loss 0.1875 | Val.Loss nan | Time(s) 1.1130\nEpoch 00144 | Loss 0.1844 | Val.Loss nan | Time(s) 1.1124\nEpoch 00145 | Loss 0.1814 | Val.Loss nan | Time(s) 1.1117\nEpoch 00146 | Loss 0.1785 | Val.Loss nan | Time(s) 1.1110\nEpoch 00147 | Loss 0.1756 | Val.Loss nan | Time(s) 1.1106\nEpoch 00148 | Loss 0.1728 | Val.Loss nan | Time(s) 1.1099\nEpoch 00149 | Loss 0.1700 | Val.Loss nan | Time(s) 1.1102\n"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "net = Net(features.shape[1], 21, len(np.unique(labels)))\n",
    "#print(net)\n",
    "\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=1e-3)\n",
    "net.train() # Set to training mode (use dropout)\n",
    "\n",
    "dur = []\n",
    "for epoch in range(150):\n",
    "    if epoch >=3:\n",
    "        t0 = time.time()\n",
    "\n",
    "    # Compute loss for test nodes (only for validation, not used by optimizer)\n",
    "    net.eval()\n",
    "    prediction = net(g, features)\n",
    "    #scores = th.tensor([F.nll_loss(prediction.detach()[1-mask][:,p], labels[1-mask]) for p in label_perms])\n",
    "    #val_loss = th.min(scores)\n",
    "    #val_loss = F.nll_loss(prediction.detach()[1-mask], labels[1-mask])\n",
    "    val_loss=th.tensor(np.nan)\n",
    "    net.train()\n",
    "\n",
    "    # Compute loss for train nodes\n",
    "    logits = net(g, features)\n",
    "\n",
    "    loss = th.tensor(1000.0,requires_grad=True)\n",
    "    for p in label_perms:\n",
    "        loss = th.min(loss,F.nll_loss(logits[mask][:,p], labels[mask]))\n",
    "\n",
    "    #loss = F.nll_loss(logits[mask], labels[mask])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >=3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Val.Loss {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), val_loss.item(), np.mean(dur)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 6],\n       [0, 6],\n       [0, 6],\n       [0, 6],\n       [0, 6],\n       [0, 6],\n       [0, 6],\n       [1, 4],\n       [1, 4],\n       [1, 4],\n       [1, 4],\n       [1, 4],\n       [1, 4],\n       [1, 4],\n       [2, 5],\n       [2, 5],\n       [3, 1],\n       [3, 1],\n       [3, 1],\n       [3, 1],\n       [3, 1],\n       [3, 1],\n       [3, 1],\n       [3, 1],\n       [3, 1],\n       [4, 3],\n       [4, 3],\n       [4, 3],\n       [4, 3],\n       [4, 3],\n       [4, 3],\n       [4, 3],\n       [4, 3],\n       [4, 3],\n       [4, 3],\n       [4, 3],\n       [5, 0],\n       [5, 0],\n       [5, 0],\n       [5, 0],\n       [5, 0],\n       [6, 2],\n       [6, 2],\n       [6, 2],\n       [6, 2],\n       [6, 2],\n       [6, 2],\n       [6, 2],\n       [6, 2],\n       [6, 2],\n       [6, 2],\n       [6, 2],\n       [6, 2],\n       [6, 2]], dtype=int64)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualise predictions\n",
    "net.eval() # Set net to evaluation mode (deactivates dropout)\n",
    "final_prediction = net(g, features).detach()\n",
    "a = np.transpose(np.vstack([final_prediction[mask].numpy().argmax(axis=1),labels[mask].numpy()]))\n",
    "a[a[:,0].argsort()]\n",
    "# as can be seen, the net predicts other labels, but gets the clusters right :)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>All</th>\n      <th>Train</th>\n      <th>Test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Mutual Information</th>\n      <td>0.335656</td>\n      <td>1.0</td>\n      <td>0.326277</td>\n    </tr>\n    <tr>\n      <th>Rand-Index</th>\n      <td>0.303419</td>\n      <td>1.0</td>\n      <td>0.294430</td>\n    </tr>\n    <tr>\n      <th>Variation of Information</th>\n      <td>2.279773</td>\n      <td>0.0</td>\n      <td>2.306509</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                              All   Train      Test\nMutual Information        0.335656    1.0  0.326277\nRand-Index                0.303419    1.0  0.294430\nVariation of Information  2.279773    0.0  2.306509"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval() # Set net to evaluation mode (deactivates dropout)\n",
    "final_prediction = net(g, features).detach()\n",
    "pf.performance_as_df(labels,final_prediction,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}