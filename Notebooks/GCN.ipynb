{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Convolutional Network by Kipf and Welling"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GNN Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModule(nn.Module):\n",
    "    \"\"\"The linear transformation part of the GCN layer\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(LinearModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation # This is the activation function\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        h = self.activation(h)\n",
    "        return {'h' : h}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\"A GCN layer\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = LinearModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(message_func=fn.copy_src(src='h', out='m'), reduce_func=fn.sum(msg='m', out='h'))\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.gcn1 = GCN(1433, 21, F.relu)\n",
    "        self.gcn2 = GCN(21, 7, lambda x: F.log_softmax(x,1))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = self.gcn1(g, features)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gcn2(g, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import citation_graph as citegrh\n",
    "import networkx as nx\n",
    "def load_cora_data():\n",
    "    data = citegrh.load_cora()\n",
    "    features = th.FloatTensor(data.features)\n",
    "    labels = th.LongTensor(data.labels)\n",
    "    mask = th.ByteTensor(data.train_mask)\n",
    "    g = data.graph\n",
    "\n",
    "    # add self loop\n",
    "    g.remove_edges_from(nx.selfloop_edges(g))\n",
    "    g = DGLGraph(g)\n",
    "    g.add_edges(g.nodes(), g.nodes())\n",
    "    \n",
    "    return g, features, labels, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_train = 0.02\n",
    "\n",
    "with open(\"data/cora_permutation1.pickle\",\"rb\") as f:\n",
    "    perm1 = pickle.load(f)\n",
    "mask = np.zeros(g.number_of_nodes())\n",
    "mask[perm1[range(int(percentage_train*g.number_of_nodes()))]] = 1\n",
    "mask = th.ByteTensor(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 00000 | Loss 1.9930 | Val.Loss 1.9919 | Time(s) nan\nEpoch 00001 | Loss 1.9497 | Val.Loss 1.9712 | Time(s) nan\nEpoch 00002 | Loss 1.8898 | Val.Loss 1.9552 | Time(s) nan\nEpoch 00003 | Loss 1.8589 | Val.Loss 1.9407 | Time(s) 0.1410\nEpoch 00004 | Loss 1.7933 | Val.Loss 1.9273 | Time(s) 0.1440\nEpoch 00005 | Loss 1.7647 | Val.Loss 1.9149 | Time(s) 0.1400\nEpoch 00006 | Loss 1.7349 | Val.Loss 1.9028 | Time(s) 0.1415\nEpoch 00007 | Loss 1.6941 | Val.Loss 1.8907 | Time(s) 0.1404\nEpoch 00008 | Loss 1.6610 | Val.Loss 1.8783 | Time(s) 0.1415\nEpoch 00009 | Loss 1.6221 | Val.Loss 1.8661 | Time(s) 0.1419\nEpoch 00010 | Loss 1.6548 | Val.Loss 1.8539 | Time(s) 0.1414\nEpoch 00011 | Loss 1.6209 | Val.Loss 1.8419 | Time(s) 0.1415\nEpoch 00012 | Loss 1.6168 | Val.Loss 1.8300 | Time(s) 0.1461\nEpoch 00013 | Loss 1.5425 | Val.Loss 1.8183 | Time(s) 0.1463\nEpoch 00014 | Loss 1.5100 | Val.Loss 1.8066 | Time(s) 0.1468\nEpoch 00015 | Loss 1.4903 | Val.Loss 1.7950 | Time(s) 0.1461\nEpoch 00016 | Loss 1.5272 | Val.Loss 1.7835 | Time(s) 0.1462\nEpoch 00017 | Loss 1.5067 | Val.Loss 1.7720 | Time(s) 0.1465\nEpoch 00018 | Loss 1.4150 | Val.Loss 1.7608 | Time(s) 0.1469\nEpoch 00019 | Loss 1.4772 | Val.Loss 1.7497 | Time(s) 0.1461\nEpoch 00020 | Loss 1.4516 | Val.Loss 1.7384 | Time(s) 0.1462\nEpoch 00021 | Loss 1.4021 | Val.Loss 1.7270 | Time(s) 0.1460\nEpoch 00022 | Loss 1.3494 | Val.Loss 1.7156 | Time(s) 0.1459\nEpoch 00023 | Loss 1.3382 | Val.Loss 1.7043 | Time(s) 0.1457\nEpoch 00024 | Loss 1.3418 | Val.Loss 1.6933 | Time(s) 0.1459\nEpoch 00025 | Loss 1.3507 | Val.Loss 1.6824 | Time(s) 0.1457\nEpoch 00026 | Loss 1.3226 | Val.Loss 1.6715 | Time(s) 0.1461\nEpoch 00027 | Loss 1.2885 | Val.Loss 1.6609 | Time(s) 0.1459\nEpoch 00028 | Loss 1.2670 | Val.Loss 1.6505 | Time(s) 0.1461\nEpoch 00029 | Loss 1.2516 | Val.Loss 1.6407 | Time(s) 0.1460\nEpoch 00030 | Loss 1.2452 | Val.Loss 1.6313 | Time(s) 0.1459\nEpoch 00031 | Loss 1.2482 | Val.Loss 1.6222 | Time(s) 0.1463\nEpoch 00032 | Loss 1.2329 | Val.Loss 1.6134 | Time(s) 0.1471\nEpoch 00033 | Loss 1.2369 | Val.Loss 1.6048 | Time(s) 0.1478\nEpoch 00034 | Loss 1.1945 | Val.Loss 1.5965 | Time(s) 0.1480\nEpoch 00035 | Loss 1.1645 | Val.Loss 1.5885 | Time(s) 0.1481\nEpoch 00036 | Loss 1.1653 | Val.Loss 1.5808 | Time(s) 0.1485\nEpoch 00037 | Loss 1.1622 | Val.Loss 1.5733 | Time(s) 0.1483\nEpoch 00038 | Loss 1.1406 | Val.Loss 1.5661 | Time(s) 0.1486\nEpoch 00039 | Loss 1.1233 | Val.Loss 1.5591 | Time(s) 0.1491\nEpoch 00040 | Loss 1.1214 | Val.Loss 1.5523 | Time(s) 0.1495\nEpoch 00041 | Loss 1.1143 | Val.Loss 1.5459 | Time(s) 0.1501\nEpoch 00042 | Loss 1.0789 | Val.Loss 1.5396 | Time(s) 0.1514\nEpoch 00043 | Loss 1.0927 | Val.Loss 1.5336 | Time(s) 0.1514\nEpoch 00044 | Loss 1.0397 | Val.Loss 1.5276 | Time(s) 0.1517\nEpoch 00045 | Loss 1.0369 | Val.Loss 1.5218 | Time(s) 0.1517\nEpoch 00046 | Loss 1.0573 | Val.Loss 1.5162 | Time(s) 0.1517\nEpoch 00047 | Loss 1.0619 | Val.Loss 1.5106 | Time(s) 0.1514\nEpoch 00048 | Loss 1.0491 | Val.Loss 1.5050 | Time(s) 0.1515\nEpoch 00049 | Loss 0.9961 | Val.Loss 1.4994 | Time(s) 0.1513\nEpoch 00050 | Loss 1.0089 | Val.Loss 1.4940 | Time(s) 0.1512\nEpoch 00051 | Loss 0.9879 | Val.Loss 1.4887 | Time(s) 0.1512\nEpoch 00052 | Loss 0.9619 | Val.Loss 1.4835 | Time(s) 0.1513\nEpoch 00053 | Loss 0.9864 | Val.Loss 1.4785 | Time(s) 0.1512\nEpoch 00054 | Loss 0.9761 | Val.Loss 1.4734 | Time(s) 0.1512\nEpoch 00055 | Loss 0.9331 | Val.Loss 1.4684 | Time(s) 0.1511\nEpoch 00056 | Loss 0.9221 | Val.Loss 1.4634 | Time(s) 0.1509\nEpoch 00057 | Loss 0.9537 | Val.Loss 1.4586 | Time(s) 0.1511\nEpoch 00058 | Loss 0.9191 | Val.Loss 1.4533 | Time(s) 0.1512\nEpoch 00059 | Loss 0.8925 | Val.Loss 1.4482 | Time(s) 0.1510\nEpoch 00060 | Loss 0.9044 | Val.Loss 1.4434 | Time(s) 0.1510\nEpoch 00061 | Loss 0.8878 | Val.Loss 1.4388 | Time(s) 0.1510\nEpoch 00062 | Loss 0.8940 | Val.Loss 1.4343 | Time(s) 0.1510\nEpoch 00063 | Loss 0.8495 | Val.Loss 1.4300 | Time(s) 0.1511\nEpoch 00064 | Loss 0.8603 | Val.Loss 1.4259 | Time(s) 0.1509\nEpoch 00065 | Loss 0.8354 | Val.Loss 1.4221 | Time(s) 0.1508\nEpoch 00066 | Loss 0.8232 | Val.Loss 1.4185 | Time(s) 0.1508\nEpoch 00067 | Loss 0.8114 | Val.Loss 1.4150 | Time(s) 0.1508\nEpoch 00068 | Loss 0.8161 | Val.Loss 1.4115 | Time(s) 0.1508\nEpoch 00069 | Loss 0.7978 | Val.Loss 1.4081 | Time(s) 0.1507\nEpoch 00070 | Loss 0.7779 | Val.Loss 1.4047 | Time(s) 0.1507\nEpoch 00071 | Loss 0.7752 | Val.Loss 1.4016 | Time(s) 0.1506\nEpoch 00072 | Loss 0.7749 | Val.Loss 1.3986 | Time(s) 0.1507\nEpoch 00073 | Loss 0.7660 | Val.Loss 1.3959 | Time(s) 0.1507\nEpoch 00074 | Loss 0.7327 | Val.Loss 1.3930 | Time(s) 0.1508\nEpoch 00075 | Loss 0.7342 | Val.Loss 1.3898 | Time(s) 0.1508\nEpoch 00076 | Loss 0.7344 | Val.Loss 1.3867 | Time(s) 0.1509\nEpoch 00077 | Loss 0.7510 | Val.Loss 1.3835 | Time(s) 0.1512\nEpoch 00078 | Loss 0.7035 | Val.Loss 1.3797 | Time(s) 0.1512\nEpoch 00079 | Loss 0.6929 | Val.Loss 1.3761 | Time(s) 0.1512\nEpoch 00080 | Loss 0.6865 | Val.Loss 1.3724 | Time(s) 0.1513\nEpoch 00081 | Loss 0.6589 | Val.Loss 1.3690 | Time(s) 0.1512\nEpoch 00082 | Loss 0.6742 | Val.Loss 1.3659 | Time(s) 0.1512\nEpoch 00083 | Loss 0.6530 | Val.Loss 1.3630 | Time(s) 0.1513\nEpoch 00084 | Loss 0.6883 | Val.Loss 1.3603 | Time(s) 0.1513\nEpoch 00085 | Loss 0.6512 | Val.Loss 1.3577 | Time(s) 0.1513\nEpoch 00086 | Loss 0.6198 | Val.Loss 1.3549 | Time(s) 0.1513\nEpoch 00087 | Loss 0.6396 | Val.Loss 1.3522 | Time(s) 0.1513\nEpoch 00088 | Loss 0.6069 | Val.Loss 1.3495 | Time(s) 0.1512\nEpoch 00089 | Loss 0.6020 | Val.Loss 1.3469 | Time(s) 0.1511\nEpoch 00090 | Loss 0.6088 | Val.Loss 1.3444 | Time(s) 0.1511\nEpoch 00091 | Loss 0.5891 | Val.Loss 1.3418 | Time(s) 0.1515\nEpoch 00092 | Loss 0.5619 | Val.Loss 1.3394 | Time(s) 0.1514\nEpoch 00093 | Loss 0.5798 | Val.Loss 1.3370 | Time(s) 0.1514\nEpoch 00094 | Loss 0.5648 | Val.Loss 1.3343 | Time(s) 0.1515\nEpoch 00095 | Loss 0.5614 | Val.Loss 1.3315 | Time(s) 0.1516\nEpoch 00096 | Loss 0.5464 | Val.Loss 1.3289 | Time(s) 0.1515\nEpoch 00097 | Loss 0.5392 | Val.Loss 1.3265 | Time(s) 0.1516\nEpoch 00098 | Loss 0.5537 | Val.Loss 1.3244 | Time(s) 0.1516\nEpoch 00099 | Loss 0.5404 | Val.Loss 1.3224 | Time(s) 0.1516\nEpoch 00100 | Loss 0.5069 | Val.Loss 1.3205 | Time(s) 0.1516\nEpoch 00101 | Loss 0.5333 | Val.Loss 1.3186 | Time(s) 0.1517\nEpoch 00102 | Loss 0.4991 | Val.Loss 1.3169 | Time(s) 0.1515\nEpoch 00103 | Loss 0.4865 | Val.Loss 1.3155 | Time(s) 0.1515\nEpoch 00104 | Loss 0.5308 | Val.Loss 1.3141 | Time(s) 0.1514\nEpoch 00105 | Loss 0.5018 | Val.Loss 1.3108 | Time(s) 0.1515\nEpoch 00106 | Loss 0.4772 | Val.Loss 1.3074 | Time(s) 0.1515\nEpoch 00107 | Loss 0.4680 | Val.Loss 1.3042 | Time(s) 0.1515\nEpoch 00108 | Loss 0.4759 | Val.Loss 1.3012 | Time(s) 0.1517\nEpoch 00109 | Loss 0.4584 | Val.Loss 1.2984 | Time(s) 0.1518\nEpoch 00110 | Loss 0.4709 | Val.Loss 1.2959 | Time(s) 0.1518\nEpoch 00111 | Loss 0.4427 | Val.Loss 1.2936 | Time(s) 0.1519\nEpoch 00112 | Loss 0.4739 | Val.Loss 1.2915 | Time(s) 0.1519\nEpoch 00113 | Loss 0.4327 | Val.Loss 1.2895 | Time(s) 0.1519\nEpoch 00114 | Loss 0.4285 | Val.Loss 1.2879 | Time(s) 0.1518\nEpoch 00115 | Loss 0.4143 | Val.Loss 1.2859 | Time(s) 0.1518\nEpoch 00116 | Loss 0.4297 | Val.Loss 1.2844 | Time(s) 0.1519\nEpoch 00117 | Loss 0.4169 | Val.Loss 1.2832 | Time(s) 0.1520\nEpoch 00118 | Loss 0.4158 | Val.Loss 1.2821 | Time(s) 0.1521\nEpoch 00119 | Loss 0.4057 | Val.Loss 1.2812 | Time(s) 0.1524\nEpoch 00120 | Loss 0.4168 | Val.Loss 1.2799 | Time(s) 0.1523\nEpoch 00121 | Loss 0.3908 | Val.Loss 1.2787 | Time(s) 0.1524\nEpoch 00122 | Loss 0.3753 | Val.Loss 1.2776 | Time(s) 0.1523\nEpoch 00123 | Loss 0.3810 | Val.Loss 1.2766 | Time(s) 0.1523\nEpoch 00124 | Loss 0.3500 | Val.Loss 1.2757 | Time(s) 0.1524\nEpoch 00125 | Loss 0.3875 | Val.Loss 1.2751 | Time(s) 0.1527\nEpoch 00126 | Loss 0.3483 | Val.Loss 1.2745 | Time(s) 0.1527\nEpoch 00127 | Loss 0.3511 | Val.Loss 1.2739 | Time(s) 0.1526\nEpoch 00128 | Loss 0.3301 | Val.Loss 1.2736 | Time(s) 0.1527\nEpoch 00129 | Loss 0.3484 | Val.Loss 1.2733 | Time(s) 0.1526\nEpoch 00130 | Loss 0.3272 | Val.Loss 1.2731 | Time(s) 0.1526\nEpoch 00131 | Loss 0.3443 | Val.Loss 1.2731 | Time(s) 0.1526\nEpoch 00132 | Loss 0.3194 | Val.Loss 1.2734 | Time(s) 0.1526\nEpoch 00133 | Loss 0.3380 | Val.Loss 1.2736 | Time(s) 0.1525\nEpoch 00134 | Loss 0.3141 | Val.Loss 1.2739 | Time(s) 0.1525\nEpoch 00135 | Loss 0.3371 | Val.Loss 1.2744 | Time(s) 0.1525\nEpoch 00136 | Loss 0.3132 | Val.Loss 1.2749 | Time(s) 0.1524\nEpoch 00137 | Loss 0.3135 | Val.Loss 1.2753 | Time(s) 0.1525\nEpoch 00138 | Loss 0.3271 | Val.Loss 1.2757 | Time(s) 0.1524\nEpoch 00139 | Loss 0.3119 | Val.Loss 1.2761 | Time(s) 0.1525\nEpoch 00140 | Loss 0.2993 | Val.Loss 1.2762 | Time(s) 0.1524\nEpoch 00141 | Loss 0.3004 | Val.Loss 1.2767 | Time(s) 0.1524\nEpoch 00142 | Loss 0.2822 | Val.Loss 1.2771 | Time(s) 0.1524\nEpoch 00143 | Loss 0.2904 | Val.Loss 1.2776 | Time(s) 0.1525\nEpoch 00144 | Loss 0.2719 | Val.Loss 1.2783 | Time(s) 0.1528\nEpoch 00145 | Loss 0.2577 | Val.Loss 1.2789 | Time(s) 0.1528\nEpoch 00146 | Loss 0.2701 | Val.Loss 1.2798 | Time(s) 0.1528\nEpoch 00147 | Loss 0.2652 | Val.Loss 1.2805 | Time(s) 0.1528\nEpoch 00148 | Loss 0.2633 | Val.Loss 1.2815 | Time(s) 0.1528\nEpoch 00149 | Loss 0.2559 | Val.Loss 1.2826 | Time(s) 0.1527\n"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "net = Net()\n",
    "#print(net)\n",
    "\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=1e-3)\n",
    "net.train() # Set to training mode (use dropout)\n",
    "\n",
    "dur = []\n",
    "for epoch in range(150):\n",
    "    if epoch >=3:\n",
    "        t0 = time.time()\n",
    "\n",
    "    # Compute loss for test nodes (only for validation, not used by optimizer)\n",
    "    net.eval()\n",
    "    prediction = net(g, features)\n",
    "    val_loss = F.nll_loss(prediction.detach()[1-mask], labels[1-mask])\n",
    "    net.train()\n",
    "\n",
    "    # Compute loss for train nodes\n",
    "    logits = net(g, features)\n",
    "    loss = F.nll_loss(logits[mask], labels[mask])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >=3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Val.Loss {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), val_loss.item(), np.mean(dur)))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nNLL-Loss:\n All : 1.2630 | Train: 0.2484 | Test: 1.2836 |\n\nAccuracy:\n All : 0.5842 | Train: 1.0000 | Test: 0.5757 |\n"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score as acc\n",
    "net.eval() # Set net to evaluation mode (deactivates dropout)\n",
    "final_prediction = net(g, features).detach()\n",
    "\n",
    "pred_sets = {\"All \":final_prediction,\"Train\":final_prediction[mask],\"Test\":final_prediction[1-mask]}\n",
    "label_sets = {\"All \":labels,\"Train\":labels[mask],\"Test\":labels[1-mask]}\n",
    "eval_functions = {\"NLL-Loss\":lambda y,x: F.nll_loss(x,y),\"Accuracy\":lambda y,x: acc(y,x.numpy().argmax(axis=1))}\n",
    "\n",
    "for name,func in eval_functions.items():\n",
    "    eval_message = f\"\\n{name}:\\n\"\n",
    "    for subset in pred_sets.keys():\n",
    "        eval_message += f\" {subset}: {func(label_sets[subset],pred_sets[subset]):.4f} |\"\n",
    "    print(eval_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}