{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Convolutional Network by Kipf and Welling"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModule(nn.Module):\n",
    "    \"\"\"The linear transformation part of the GCN layer\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(LinearModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation # This is the activation function\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        h = self.activation(h)\n",
    "        return {'h' : h}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\"A GCN layer\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = LinearModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(message_func=fn.copy_src(src='h', out='m'), reduce_func=fn.sum(msg='m', out='h'))\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.gcn1 = GCN(1433, 16, F.relu)\n",
    "        self.gcn2 = GCN(16, 7, lambda x: F.log_softmax(x,1))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = self.gcn1(g, features)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gcn2(g, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import citation_graph as citegrh\n",
    "import networkx as nx\n",
    "def load_cora_data():\n",
    "    data = citegrh.load_cora()\n",
    "    features = th.FloatTensor(data.features)\n",
    "    labels = th.LongTensor(data.labels)\n",
    "    mask = th.ByteTensor(data.train_mask)\n",
    "    g = data.graph\n",
    "\n",
    "    # add self loop\n",
    "    g.remove_edges_from(nx.selfloop_edges(g))\n",
    "    g = DGLGraph(g)\n",
    "    g.add_edges(g.nodes(), g.nodes())\n",
    "    \n",
    "    return g, features, labels, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 0, 0,  ..., 1, 1, 1], dtype=torch.uint8)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#g, features, labels, mask = load_cora_data()\n",
    "1-mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 00000 | Loss 1.9975 | Val.Loss 1.9568 | Time(s) nan\nEpoch 00001 | Loss 1.9753 | Val.Loss 1.9422 | Time(s) nan\nEpoch 00002 | Loss 1.9476 | Val.Loss 1.9287 | Time(s) nan\nEpoch 00003 | Loss 1.9187 | Val.Loss 1.9169 | Time(s) 0.1580\nEpoch 00004 | Loss 1.9089 | Val.Loss 1.9053 | Time(s) 0.2475\nEpoch 00005 | Loss 1.8851 | Val.Loss 1.8934 | Time(s) 0.2233\nEpoch 00006 | Loss 1.8574 | Val.Loss 1.8813 | Time(s) 0.2180\nEpoch 00007 | Loss 1.8291 | Val.Loss 1.8692 | Time(s) 0.2178\nEpoch 00008 | Loss 1.7967 | Val.Loss 1.8577 | Time(s) 0.2153\nEpoch 00009 | Loss 1.7946 | Val.Loss 1.8455 | Time(s) 0.2093\nEpoch 00010 | Loss 1.7597 | Val.Loss 1.8330 | Time(s) 0.2285\nEpoch 00011 | Loss 1.7313 | Val.Loss 1.8202 | Time(s) 0.2270\nEpoch 00012 | Loss 1.7301 | Val.Loss 1.8065 | Time(s) 0.2229\nEpoch 00013 | Loss 1.6733 | Val.Loss 1.7921 | Time(s) 0.2355\nEpoch 00014 | Loss 1.6348 | Val.Loss 1.7772 | Time(s) 0.2303\nEpoch 00015 | Loss 1.6543 | Val.Loss 1.7623 | Time(s) 0.2289\nEpoch 00016 | Loss 1.6279 | Val.Loss 1.7478 | Time(s) 0.2307\nEpoch 00017 | Loss 1.5785 | Val.Loss 1.7338 | Time(s) 0.2294\nEpoch 00018 | Loss 1.5497 | Val.Loss 1.7199 | Time(s) 0.2285\nEpoch 00019 | Loss 1.5598 | Val.Loss 1.7061 | Time(s) 0.2284\nEpoch 00020 | Loss 1.5047 | Val.Loss 1.6923 | Time(s) 0.2258\nEpoch 00021 | Loss 1.4769 | Val.Loss 1.6789 | Time(s) 0.2332\nEpoch 00022 | Loss 1.4805 | Val.Loss 1.6658 | Time(s) 0.2310\nEpoch 00023 | Loss 1.4617 | Val.Loss 1.6530 | Time(s) 0.2291\nEpoch 00024 | Loss 1.4650 | Val.Loss 1.6405 | Time(s) 0.2288\nEpoch 00025 | Loss 1.4229 | Val.Loss 1.6283 | Time(s) 0.2274\nEpoch 00026 | Loss 1.4060 | Val.Loss 1.6163 | Time(s) 0.2261\nEpoch 00027 | Loss 1.3870 | Val.Loss 1.6047 | Time(s) 0.2256\nEpoch 00028 | Loss 1.3665 | Val.Loss 1.5930 | Time(s) 0.2264\nEpoch 00029 | Loss 1.3176 | Val.Loss 1.5817 | Time(s) 0.2320\nEpoch 00030 | Loss 1.3511 | Val.Loss 1.5706 | Time(s) 0.2303\nEpoch 00031 | Loss 1.3381 | Val.Loss 1.5595 | Time(s) 0.2311\nEpoch 00032 | Loss 1.3244 | Val.Loss 1.5487 | Time(s) 0.2304\nEpoch 00033 | Loss 1.2801 | Val.Loss 1.5383 | Time(s) 0.2292\nEpoch 00034 | Loss 1.2373 | Val.Loss 1.5281 | Time(s) 0.2278\nEpoch 00035 | Loss 1.2596 | Val.Loss 1.5180 | Time(s) 0.2262\nEpoch 00036 | Loss 1.2163 | Val.Loss 1.5080 | Time(s) 0.2253\nEpoch 00037 | Loss 1.2209 | Val.Loss 1.4981 | Time(s) 0.2293\nEpoch 00038 | Loss 1.2346 | Val.Loss 1.4885 | Time(s) 0.2282\nEpoch 00039 | Loss 1.1690 | Val.Loss 1.4791 | Time(s) 0.2272\nEpoch 00040 | Loss 1.1490 | Val.Loss 1.4699 | Time(s) 0.2260\nEpoch 00041 | Loss 1.1392 | Val.Loss 1.4608 | Time(s) 0.2246\nEpoch 00042 | Loss 1.1369 | Val.Loss 1.4517 | Time(s) 0.2233\nEpoch 00043 | Loss 1.1405 | Val.Loss 1.4428 | Time(s) 0.2226\nEpoch 00044 | Loss 1.1055 | Val.Loss 1.4339 | Time(s) 0.2219\nEpoch 00045 | Loss 1.0894 | Val.Loss 1.4253 | Time(s) 0.2217\nEpoch 00046 | Loss 1.1175 | Val.Loss 1.4167 | Time(s) 0.2246\nEpoch 00047 | Loss 1.0724 | Val.Loss 1.4081 | Time(s) 0.2233\nEpoch 00048 | Loss 1.0443 | Val.Loss 1.3996 | Time(s) 0.2229\nEpoch 00049 | Loss 1.0451 | Val.Loss 1.3913 | Time(s) 0.2217\nEpoch 00050 | Loss 1.0205 | Val.Loss 1.3831 | Time(s) 0.2210\nEpoch 00051 | Loss 1.0433 | Val.Loss 1.3751 | Time(s) 0.2203\nEpoch 00052 | Loss 1.0002 | Val.Loss 1.3674 | Time(s) 0.2197\nEpoch 00053 | Loss 1.0136 | Val.Loss 1.3598 | Time(s) 0.2195\nEpoch 00054 | Loss 1.0324 | Val.Loss 1.3522 | Time(s) 0.2190\nEpoch 00055 | Loss 0.9696 | Val.Loss 1.3448 | Time(s) 0.2188\nEpoch 00056 | Loss 0.9628 | Val.Loss 1.3374 | Time(s) 0.2179\nEpoch 00057 | Loss 0.9537 | Val.Loss 1.3301 | Time(s) 0.2169\nEpoch 00058 | Loss 0.9272 | Val.Loss 1.3231 | Time(s) 0.2193\nEpoch 00059 | Loss 0.9218 | Val.Loss 1.3161 | Time(s) 0.2194\nEpoch 00060 | Loss 0.9299 | Val.Loss 1.3094 | Time(s) 0.2187\nEpoch 00061 | Loss 0.9172 | Val.Loss 1.3027 | Time(s) 0.2180\nEpoch 00062 | Loss 0.9396 | Val.Loss 1.2962 | Time(s) 0.2176\nEpoch 00063 | Loss 0.8808 | Val.Loss 1.2897 | Time(s) 0.2176\nEpoch 00064 | Loss 0.8746 | Val.Loss 1.2835 | Time(s) 0.2172\nEpoch 00065 | Loss 0.8801 | Val.Loss 1.2774 | Time(s) 0.2172\nEpoch 00066 | Loss 0.8671 | Val.Loss 1.2713 | Time(s) 0.2170\nEpoch 00067 | Loss 0.8854 | Val.Loss 1.2653 | Time(s) 0.2170\nEpoch 00068 | Loss 0.8575 | Val.Loss 1.2594 | Time(s) 0.2165\nEpoch 00069 | Loss 0.8471 | Val.Loss 1.2536 | Time(s) 0.2161\nEpoch 00070 | Loss 0.8579 | Val.Loss 1.2479 | Time(s) 0.2157\nEpoch 00071 | Loss 0.8117 | Val.Loss 1.2423 | Time(s) 0.2152\nEpoch 00072 | Loss 0.8310 | Val.Loss 1.2368 | Time(s) 0.2171\nEpoch 00073 | Loss 0.8088 | Val.Loss 1.2316 | Time(s) 0.2166\nEpoch 00074 | Loss 0.8274 | Val.Loss 1.2265 | Time(s) 0.2166\nEpoch 00075 | Loss 0.8149 | Val.Loss 1.2215 | Time(s) 0.2165\nEpoch 00076 | Loss 0.7909 | Val.Loss 1.2166 | Time(s) 0.2163\nEpoch 00077 | Loss 0.7794 | Val.Loss 1.2118 | Time(s) 0.2157\nEpoch 00078 | Loss 0.7863 | Val.Loss 1.2070 | Time(s) 0.2173\nEpoch 00079 | Loss 0.7758 | Val.Loss 1.2023 | Time(s) 0.2168\nEpoch 00080 | Loss 0.7635 | Val.Loss 1.1976 | Time(s) 0.2163\nEpoch 00081 | Loss 0.7385 | Val.Loss 1.1930 | Time(s) 0.2159\nEpoch 00082 | Loss 0.7544 | Val.Loss 1.1883 | Time(s) 0.2152\nEpoch 00083 | Loss 0.7505 | Val.Loss 1.1838 | Time(s) 0.2155\nEpoch 00084 | Loss 0.7384 | Val.Loss 1.1793 | Time(s) 0.2153\nEpoch 00085 | Loss 0.7420 | Val.Loss 1.1750 | Time(s) 0.2172\nEpoch 00086 | Loss 0.7438 | Val.Loss 1.1709 | Time(s) 0.2169\nEpoch 00087 | Loss 0.7114 | Val.Loss 1.1669 | Time(s) 0.2166\nEpoch 00088 | Loss 0.7034 | Val.Loss 1.1630 | Time(s) 0.2162\nEpoch 00089 | Loss 0.7007 | Val.Loss 1.1592 | Time(s) 0.2159\n"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "net = Net()\n",
    "#print(net)\n",
    "\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=1e-3)\n",
    "net.train() # Set to training mode (use dropout)\n",
    "\n",
    "dur = []\n",
    "for epoch in range(90):\n",
    "    if epoch >=3:\n",
    "        t0 = time.time()\n",
    "\n",
    "    net.eval()\n",
    "    prediction = net(g, features)\n",
    "    val_loss = F.nll_loss(prediction.detach()[1-mask], labels[1-mask])\n",
    "    net.train()\n",
    "    #val_loss = F.nll_loss(logits.detach()[1-mask], labels[1-mask])\n",
    "\n",
    "    logits = net(g, features)\n",
    "    loss = F.nll_loss(logits[mask], labels[mask])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >=3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Val.Loss {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), val_loss.item(), np.mean(dur)))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nNLL-Loss:\n All : 1.1305 | Train: 0.6747 | Test: 1.1553 |\n\nAccuracy:\n All : 0.7740 | Train: 0.9571 | Test: 0.7640 |\n"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score as acc\n",
    "net.eval() # Set net to evaluation mode (deactivates dropout)\n",
    "final_prediction = net(g, features).detach()\n",
    "\n",
    "pred_sets = {\"All \":final_prediction,\"Train\":final_prediction[mask],\"Test\":final_prediction[1-mask]}\n",
    "label_sets = {\"All \":labels,\"Train\":labels[mask],\"Test\":labels[1-mask]}\n",
    "eval_functions = {\"NLL-Loss\":lambda y,x: F.nll_loss(x,y),\"Accuracy\":lambda y,x: acc(y,x.numpy().argmax(axis=1))}\n",
    "\n",
    "for name,func in eval_functions.items():\n",
    "    eval_message = f\"\\n{name}:\\n\"\n",
    "    for subset in pred_sets.keys():\n",
    "        eval_message += f\" {subset}: {func(label_sets[subset],pred_sets[subset]):.4f} |\"\n",
    "    print(eval_message)"
   ]
  }
 ]
}