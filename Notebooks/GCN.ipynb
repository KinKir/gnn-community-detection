{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Convolutional Network by Kipf and Welling"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GNN Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModule(nn.Module):\n",
    "    \"\"\"The linear transformation part of the GCN layer\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(LinearModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation # This is the activation function\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        h = self.activation(h)\n",
    "        return {'h' : h}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\"A GCN layer\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = LinearModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(message_func=fn.copy_src(src='h', out='m'), reduce_func=fn.sum(msg='m', out='h'))\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, infeats, hidden_size, outfeats):\n",
    "        super(Net, self).__init__()\n",
    "        self.gcn1 = GCN(infeats, hidden_size, F.relu)\n",
    "        self.gcn2 = GCN(hidden_size, outfeats, lambda x: F.log_softmax(x,1))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = self.gcn1(g, features)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.gcn2(g, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import citation_graph as citegrh\n",
    "import networkx as nx\n",
    "\n",
    "data = citegrh.load_cora()\n",
    "features = th.FloatTensor(data.features)\n",
    "labels = th.LongTensor(data.labels)\n",
    "mask = th.ByteTensor(data.train_mask)\n",
    "g = data.graph\n",
    "\n",
    "# add self loop\n",
    "g.remove_edges_from(nx.selfloop_edges(g))\n",
    "g = DGLGraph(g)\n",
    "g.add_edges(g.nodes(), g.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_train = 0.02\n",
    "\n",
    "with open(\"data/cora_permutation1.pickle\",\"rb\") as f:\n",
    "    perm1 = pickle.load(f)\n",
    "mask = np.zeros(g.number_of_nodes())\n",
    "mask[perm1[range(int(percentage_train*g.number_of_nodes()))]] = 1\n",
    "mask = th.ByteTensor(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_perms = list(itertools.permutations(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Users\\a_liso02\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n  out=out, **kwargs)\nC:\\Users\\a_liso02\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\nEpoch 00000 | Loss 1.8873 | Val.Loss nan | Time(s) nan\nEpoch 00001 | Loss 1.8493 | Val.Loss nan | Time(s) nan\nEpoch 00002 | Loss 1.8167 | Val.Loss nan | Time(s) nan\nEpoch 00003 | Loss 1.7839 | Val.Loss nan | Time(s) 1.1010\nEpoch 00004 | Loss 1.7513 | Val.Loss nan | Time(s) 1.0890\nEpoch 00005 | Loss 1.7169 | Val.Loss nan | Time(s) 1.0820\nEpoch 00006 | Loss 1.6804 | Val.Loss nan | Time(s) 1.0748\nEpoch 00007 | Loss 1.6444 | Val.Loss nan | Time(s) 1.0718\nEpoch 00008 | Loss 1.6092 | Val.Loss nan | Time(s) 1.0697\nEpoch 00009 | Loss 1.5762 | Val.Loss nan | Time(s) 1.0659\nEpoch 00010 | Loss 1.5458 | Val.Loss nan | Time(s) 1.0678\nEpoch 00011 | Loss 1.5176 | Val.Loss nan | Time(s) 1.0680\nEpoch 00012 | Loss 1.4912 | Val.Loss nan | Time(s) 1.0664\nEpoch 00013 | Loss 1.4661 | Val.Loss nan | Time(s) 1.0647\nEpoch 00014 | Loss 1.4419 | Val.Loss nan | Time(s) 1.0656\nEpoch 00015 | Loss 1.4187 | Val.Loss nan | Time(s) 1.0623\nEpoch 00016 | Loss 1.3963 | Val.Loss nan | Time(s) 1.0613\nEpoch 00017 | Loss 1.3745 | Val.Loss nan | Time(s) 1.0585\nEpoch 00018 | Loss 1.3533 | Val.Loss nan | Time(s) 1.0662\nEpoch 00019 | Loss 1.3327 | Val.Loss nan | Time(s) 1.0634\nEpoch 00020 | Loss 1.3128 | Val.Loss nan | Time(s) 1.0623\nEpoch 00021 | Loss 1.2935 | Val.Loss nan | Time(s) 1.0615\nEpoch 00022 | Loss 1.2750 | Val.Loss nan | Time(s) 1.0595\nEpoch 00023 | Loss 1.2569 | Val.Loss nan | Time(s) 1.0593\n"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "net = Net(features.shape[1], 21, len(np.unique(labels)))\n",
    "#print(net)\n",
    "\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=1e-3)\n",
    "net.train() # Set to training mode (use dropout)\n",
    "\n",
    "dur = []\n",
    "for epoch in range(150):\n",
    "    if epoch >=3:\n",
    "        t0 = time.time()\n",
    "\n",
    "    # Compute loss for test nodes (only for validation, not used by optimizer)\n",
    "    net.eval()\n",
    "    prediction = net(g, features)\n",
    "    #scores = th.tensor([F.nll_loss(prediction.detach()[1-mask][:,p], labels[1-mask]) for p in label_perms])\n",
    "    #val_loss = th.min(scores)\n",
    "    #val_loss = F.nll_loss(prediction.detach()[1-mask], labels[1-mask])\n",
    "    val_loss=th.tensor(np.nan)\n",
    "    net.train()\n",
    "\n",
    "    # Compute loss for train nodes\n",
    "    logits = net(g, features)\n",
    "\n",
    "    loss = th.tensor(1000.0,requires_grad=True)\n",
    "    for p in label_perms:\n",
    "        loss = th.min(loss,F.nll_loss(logits[mask][:,p], labels[mask]))\n",
    "\n",
    "    #loss = F.nll_loss(logits[mask], labels[mask])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >=3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Val.Loss {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), val_loss.item(), np.mean(dur)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 4],\n       [0, 4],\n       [0, 4],\n       [0, 4],\n       [0, 4],\n       [0, 4],\n       [0, 4],\n       [1, 0],\n       [1, 0],\n       [1, 0],\n       [1, 0],\n       [1, 0],\n       [2, 5],\n       [2, 5],\n       [3, 3],\n       [3, 3],\n       [3, 3],\n       [3, 3],\n       [3, 3],\n       [3, 3],\n       [3, 3],\n       [3, 3],\n       [3, 3],\n       [3, 3],\n       [3, 3],\n       [4, 2],\n       [4, 2],\n       [4, 2],\n       [4, 2],\n       [4, 2],\n       [4, 2],\n       [4, 2],\n       [4, 2],\n       [4, 2],\n       [4, 2],\n       [4, 2],\n       [4, 2],\n       [4, 2],\n       [5, 1],\n       [5, 1],\n       [5, 1],\n       [5, 1],\n       [5, 1],\n       [5, 1],\n       [5, 1],\n       [5, 1],\n       [5, 1],\n       [6, 6],\n       [6, 6],\n       [6, 6],\n       [6, 6],\n       [6, 6],\n       [6, 6],\n       [6, 6]], dtype=int64)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualise predictions\n",
    "net.eval() # Set net to evaluation mode (deactivates dropout)\n",
    "final_prediction = net(g, features).detach()\n",
    "a = np.transpose(np.vstack([final_prediction[mask].numpy().argmax(axis=1),labels[mask].numpy()]))\n",
    "a[a[:,0].argsort()]\n",
    "# as can be seen, the net predicts other labels, but gets the clusters right :)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nNLL-Loss:\n All : 3.9184 | Train: 4.2528 | Test: 3.9116 |\n\nAccuracy:\n All : 0.2097 | Train: 0.3333 | Test: 0.2072 |\n"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score as acc\n",
    "net.eval() # Set net to evaluation mode (deactivates dropout)\n",
    "final_prediction = net(g, features).detach()\n",
    "\n",
    "pred_sets = {\"All \":final_prediction,\"Train\":final_prediction[mask],\"Test\":final_prediction[1-mask]}\n",
    "label_sets = {\"All \":labels,\"Train\":labels[mask],\"Test\":labels[1-mask]}\n",
    "eval_functions = {\"NLL-Loss\":lambda y,x: F.nll_loss(x,y),\"Accuracy\":lambda y,x: acc(y,x.numpy().argmax(axis=1))}\n",
    "\n",
    "for name,func in eval_functions.items():\n",
    "    eval_message = f\"\\n{name}:\\n\"\n",
    "    for subset in pred_sets.keys():\n",
    "        eval_message += f\" {subset}: {func(label_sets[subset],pred_sets[subset]):.4f} |\"\n",
    "    print(eval_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}