{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Convolutional Network by Kipf and Welling"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModule(nn.Module):\n",
    "    \"\"\"The linear transformation part of the GCN layer\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(LinearModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation # This is the activation function\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        h = self.activation(h)\n",
    "        return {'h' : h}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\"A GCN layer\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = LinearModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(message_func=fn.copy_src(src='h', out='m'), reduce_func=fn.sum(msg='m', out='h'))\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.gcn1 = GCN(1433, 16, F.relu)\n",
    "        self.gcn2 = GCN(16, 7, lambda x: F.log_softmax(x,1))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = self.gcn1(g, features)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gcn2(g, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import citation_graph as citegrh\n",
    "import networkx as nx\n",
    "def load_cora_data():\n",
    "    data = citegrh.load_cora()\n",
    "    features = th.FloatTensor(data.features)\n",
    "    labels = th.LongTensor(data.labels)\n",
    "    mask = th.ByteTensor(data.train_mask)\n",
    "    g = data.graph\n",
    "\n",
    "    # add self loop\n",
    "    g.remove_edges_from(nx.selfloop_edges(g))\n",
    "    g = DGLGraph(g)\n",
    "    g.add_edges(g.nodes(), g.nodes())\n",
    "    \n",
    "    return g, features, labels, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make Random Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_train = 0.02\n",
    "mask = th.ByteTensor(np.random.choice([0,1],p=[1-percentage_train,percentage_train],size=g.number_of_nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 0, 0,  ..., 1, 1, 1], dtype=torch.uint8)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#g, features, labels, mask = load_cora_data()\n",
    "1-mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Users\\a_liso02\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n  out=out, **kwargs)\nC:\\Users\\a_liso02\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\nEpoch 00000 | Loss 1.9750 | Val.Loss 2.0118 | Time(s) nan\nEpoch 00001 | Loss 1.9603 | Val.Loss 1.9939 | Time(s) nan\nEpoch 00002 | Loss 1.9336 | Val.Loss 1.9783 | Time(s) nan\nEpoch 00003 | Loss 1.9071 | Val.Loss 1.9630 | Time(s) 0.2040\nEpoch 00004 | Loss 1.8915 | Val.Loss 1.9474 | Time(s) 0.3155\nEpoch 00005 | Loss 1.8682 | Val.Loss 1.9320 | Time(s) 0.2807\nEpoch 00006 | Loss 1.8488 | Val.Loss 1.9172 | Time(s) 0.2673\nEpoch 00007 | Loss 1.8247 | Val.Loss 1.9022 | Time(s) 0.2724\nEpoch 00008 | Loss 1.8144 | Val.Loss 1.8868 | Time(s) 0.2545\nEpoch 00009 | Loss 1.7781 | Val.Loss 1.8717 | Time(s) 0.2713\nEpoch 00010 | Loss 1.7512 | Val.Loss 1.8566 | Time(s) 0.2636\nEpoch 00011 | Loss 1.7334 | Val.Loss 1.8422 | Time(s) 0.2541\nEpoch 00012 | Loss 1.7093 | Val.Loss 1.8280 | Time(s) 0.2476\nEpoch 00013 | Loss 1.6903 | Val.Loss 1.8142 | Time(s) 0.2396\nEpoch 00014 | Loss 1.6657 | Val.Loss 1.8007 | Time(s) 0.2521\nEpoch 00015 | Loss 1.6521 | Val.Loss 1.7879 | Time(s) 0.2489\nEpoch 00016 | Loss 1.6406 | Val.Loss 1.7755 | Time(s) 0.2424\nEpoch 00017 | Loss 1.6051 | Val.Loss 1.7634 | Time(s) 0.2381\nEpoch 00018 | Loss 1.5743 | Val.Loss 1.7521 | Time(s) 0.2356\nEpoch 00019 | Loss 1.5683 | Val.Loss 1.7413 | Time(s) 0.2331\nEpoch 00020 | Loss 1.5472 | Val.Loss 1.7306 | Time(s) 0.2311\nEpoch 00021 | Loss 1.5071 | Val.Loss 1.7199 | Time(s) 0.2295\nEpoch 00022 | Loss 1.4764 | Val.Loss 1.7091 | Time(s) 0.2365\nEpoch 00023 | Loss 1.4820 | Val.Loss 1.6981 | Time(s) 0.2350\nEpoch 00024 | Loss 1.4517 | Val.Loss 1.6874 | Time(s) 0.2326\nEpoch 00025 | Loss 1.4306 | Val.Loss 1.6766 | Time(s) 0.2307\nEpoch 00026 | Loss 1.4186 | Val.Loss 1.6662 | Time(s) 0.2283\nEpoch 00027 | Loss 1.3915 | Val.Loss 1.6563 | Time(s) 0.2265\nEpoch 00028 | Loss 1.3870 | Val.Loss 1.6468 | Time(s) 0.2243\nEpoch 00029 | Loss 1.3775 | Val.Loss 1.6378 | Time(s) 0.2229\nEpoch 00030 | Loss 1.3588 | Val.Loss 1.6286 | Time(s) 0.2225\nEpoch 00031 | Loss 1.3334 | Val.Loss 1.6198 | Time(s) 0.2214\nEpoch 00032 | Loss 1.3086 | Val.Loss 1.6111 | Time(s) 0.2203\nEpoch 00033 | Loss 1.2885 | Val.Loss 1.6028 | Time(s) 0.2268\nEpoch 00034 | Loss 1.2922 | Val.Loss 1.5947 | Time(s) 0.2265\nEpoch 00035 | Loss 1.2621 | Val.Loss 1.5870 | Time(s) 0.2257\nEpoch 00036 | Loss 1.2577 | Val.Loss 1.5795 | Time(s) 0.2256\nEpoch 00037 | Loss 1.2311 | Val.Loss 1.5721 | Time(s) 0.2241\nEpoch 00038 | Loss 1.2169 | Val.Loss 1.5648 | Time(s) 0.2228\nEpoch 00039 | Loss 1.1813 | Val.Loss 1.5577 | Time(s) 0.2217\nEpoch 00040 | Loss 1.2000 | Val.Loss 1.5503 | Time(s) 0.2217\nEpoch 00041 | Loss 1.1467 | Val.Loss 1.5429 | Time(s) 0.2209\nEpoch 00042 | Loss 1.1495 | Val.Loss 1.5356 | Time(s) 0.2200\nEpoch 00043 | Loss 1.1295 | Val.Loss 1.5283 | Time(s) 0.2237\nEpoch 00044 | Loss 1.1434 | Val.Loss 1.5209 | Time(s) 0.2227\nEpoch 00045 | Loss 1.1173 | Val.Loss 1.5139 | Time(s) 0.2223\nEpoch 00046 | Loss 1.1009 | Val.Loss 1.5070 | Time(s) 0.2223\nEpoch 00047 | Loss 1.1010 | Val.Loss 1.5003 | Time(s) 0.2270\nEpoch 00048 | Loss 1.0593 | Val.Loss 1.4939 | Time(s) 0.2263\nEpoch 00049 | Loss 1.0657 | Val.Loss 1.4875 | Time(s) 0.2256\nEpoch 00050 | Loss 1.0498 | Val.Loss 1.4819 | Time(s) 0.2249\nEpoch 00051 | Loss 1.0273 | Val.Loss 1.4770 | Time(s) 0.2239\nEpoch 00052 | Loss 0.9894 | Val.Loss 1.4721 | Time(s) 0.2268\nEpoch 00053 | Loss 0.9917 | Val.Loss 1.4670 | Time(s) 0.2260\nEpoch 00054 | Loss 0.9896 | Val.Loss 1.4623 | Time(s) 0.2249\nEpoch 00055 | Loss 0.9630 | Val.Loss 1.4574 | Time(s) 0.2238\nEpoch 00056 | Loss 0.9727 | Val.Loss 1.4524 | Time(s) 0.2240\nEpoch 00057 | Loss 0.9445 | Val.Loss 1.4475 | Time(s) 0.2235\nEpoch 00058 | Loss 0.9752 | Val.Loss 1.4426 | Time(s) 0.2227\nEpoch 00059 | Loss 0.9407 | Val.Loss 1.4369 | Time(s) 0.2252\nEpoch 00060 | Loss 0.9281 | Val.Loss 1.4318 | Time(s) 0.2243\nEpoch 00061 | Loss 0.9138 | Val.Loss 1.4269 | Time(s) 0.2234\nEpoch 00062 | Loss 0.8631 | Val.Loss 1.4221 | Time(s) 0.2229\nEpoch 00063 | Loss 0.9114 | Val.Loss 1.4169 | Time(s) 0.2224\nEpoch 00064 | Loss 0.8555 | Val.Loss 1.4119 | Time(s) 0.2227\nEpoch 00065 | Loss 0.8890 | Val.Loss 1.4072 | Time(s) 0.2225\nEpoch 00066 | Loss 0.8443 | Val.Loss 1.4029 | Time(s) 0.2218\nEpoch 00067 | Loss 0.8246 | Val.Loss 1.3988 | Time(s) 0.2240\nEpoch 00068 | Loss 0.8560 | Val.Loss 1.3948 | Time(s) 0.2235\nEpoch 00069 | Loss 0.8423 | Val.Loss 1.3907 | Time(s) 0.2230\nEpoch 00070 | Loss 0.8095 | Val.Loss 1.3866 | Time(s) 0.2225\nEpoch 00071 | Loss 0.7825 | Val.Loss 1.3826 | Time(s) 0.2219\nEpoch 00072 | Loss 0.7678 | Val.Loss 1.3784 | Time(s) 0.2222\nEpoch 00073 | Loss 0.7792 | Val.Loss 1.3742 | Time(s) 0.2222\nEpoch 00074 | Loss 0.7558 | Val.Loss 1.3704 | Time(s) 0.2219\nEpoch 00075 | Loss 0.7553 | Val.Loss 1.3667 | Time(s) 0.2212\nEpoch 00076 | Loss 0.7624 | Val.Loss 1.3632 | Time(s) 0.2206\nEpoch 00077 | Loss 0.7408 | Val.Loss 1.3605 | Time(s) 0.2200\nEpoch 00078 | Loss 0.7134 | Val.Loss 1.3578 | Time(s) 0.2212\nEpoch 00079 | Loss 0.7405 | Val.Loss 1.3549 | Time(s) 0.2211\nEpoch 00080 | Loss 0.6968 | Val.Loss 1.3515 | Time(s) 0.2205\nEpoch 00081 | Loss 0.6999 | Val.Loss 1.3481 | Time(s) 0.2200\nEpoch 00082 | Loss 0.7070 | Val.Loss 1.3448 | Time(s) 0.2195\nEpoch 00083 | Loss 0.7076 | Val.Loss 1.3416 | Time(s) 0.2192\nEpoch 00084 | Loss 0.6673 | Val.Loss 1.3386 | Time(s) 0.2193\nEpoch 00085 | Loss 0.6474 | Val.Loss 1.3357 | Time(s) 0.2188\nEpoch 00086 | Loss 0.6689 | Val.Loss 1.3331 | Time(s) 0.2183\nEpoch 00087 | Loss 0.6803 | Val.Loss 1.3306 | Time(s) 0.2194\nEpoch 00088 | Loss 0.6302 | Val.Loss 1.3283 | Time(s) 0.2190\nEpoch 00089 | Loss 0.6319 | Val.Loss 1.3260 | Time(s) 0.2188\n"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "net = Net()\n",
    "#print(net)\n",
    "\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=1e-3)\n",
    "net.train() # Set to training mode (use dropout)\n",
    "\n",
    "dur = []\n",
    "for epoch in range(90):\n",
    "    if epoch >=3:\n",
    "        t0 = time.time()\n",
    "\n",
    "    # Compute loss for test nodes (only for validation, not used by optimizer)\n",
    "    net.eval()\n",
    "    prediction = net(g, features)\n",
    "    val_loss = F.nll_loss(prediction.detach()[1-mask], labels[1-mask])\n",
    "    net.train()\n",
    "\n",
    "    # Compute loss for train nodes\n",
    "    logits = net(g, features)\n",
    "    loss = F.nll_loss(logits[mask], labels[mask])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >=3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Val.Loss {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), val_loss.item(), np.mean(dur)))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nNLL-Loss:\n All : 1.3105 | Train: 0.6228 | Test: 1.3240 |\n\nAccuracy:\n All : 0.5890 | Train: 1.0000 | Test: 0.5809 |\n"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score as acc\n",
    "net.eval() # Set net to evaluation mode (deactivates dropout)\n",
    "final_prediction = net(g, features).detach()\n",
    "\n",
    "pred_sets = {\"All \":final_prediction,\"Train\":final_prediction[mask],\"Test\":final_prediction[1-mask]}\n",
    "label_sets = {\"All \":labels,\"Train\":labels[mask],\"Test\":labels[1-mask]}\n",
    "eval_functions = {\"NLL-Loss\":lambda y,x: F.nll_loss(x,y),\"Accuracy\":lambda y,x: acc(y,x.numpy().argmax(axis=1))}\n",
    "\n",
    "for name,func in eval_functions.items():\n",
    "    eval_message = f\"\\n{name}:\\n\"\n",
    "    for subset in pred_sets.keys():\n",
    "        eval_message += f\" {subset}: {func(label_sets[subset],pred_sets[subset]):.4f} |\"\n",
    "    print(eval_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}