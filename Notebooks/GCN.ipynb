{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Convolutional Network by Kipf and Welling"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModule(nn.Module):\n",
    "    \"\"\"The linear transformation part of the GCN layer\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(LinearModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation # This is the activation function\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        h = self.activation(h)\n",
    "        return {'h' : h}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\"A GCN layer\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = LinearModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(message_func=fn.copy_src(src='h', out='m'), reduce_func=fn.sum(msg='m', out='h'))\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.gcn1 = GCN(1433, 16, F.relu)\n",
    "        self.gcn2 = GCN(16, 7, lambda x: F.log_softmax(x,1))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = self.gcn1(g, features)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gcn2(g, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import citation_graph as citegrh\n",
    "import networkx as nx\n",
    "def load_cora_data():\n",
    "    data = citegrh.load_cora()\n",
    "    features = th.FloatTensor(data.features)\n",
    "    labels = th.LongTensor(data.labels)\n",
    "    mask = th.ByteTensor(data.train_mask)\n",
    "    g = data.graph\n",
    "\n",
    "    # add self loop\n",
    "    g.remove_edges_from(nx.selfloop_edges(g))\n",
    "    g = DGLGraph(g)\n",
    "    g.add_edges(g.nodes(), g.nodes())\n",
    "    \n",
    "    return g, features, labels, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_train = 0.02\n",
    "\n",
    "with open(\"data/cora_permutation1.pickle\",\"rb\") as f:\n",
    "    perm1 = pickle.load(f)\n",
    "mask = np.zeros(g.number_of_nodes())\n",
    "mask[perm1[range(int(percentage_train*g.number_of_nodes()))]] = 1\n",
    "mask = th.ByteTensor(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Users\\a_liso02\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n  out=out, **kwargs)\nC:\\Users\\a_liso02\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\nEpoch 00000 | Loss 1.9274 | Val.Loss 1.9411 | Time(s) nan\nEpoch 00001 | Loss 1.8792 | Val.Loss 1.9257 | Time(s) nan\nEpoch 00002 | Loss 1.8468 | Val.Loss 1.9139 | Time(s) nan\nEpoch 00003 | Loss 1.8213 | Val.Loss 1.9037 | Time(s) 0.1520\nEpoch 00004 | Loss 1.7917 | Val.Loss 1.8939 | Time(s) 0.1800\nEpoch 00005 | Loss 1.7971 | Val.Loss 1.8837 | Time(s) 0.1743\nEpoch 00006 | Loss 1.7355 | Val.Loss 1.8731 | Time(s) 0.2178\nEpoch 00007 | Loss 1.6957 | Val.Loss 1.8621 | Time(s) 0.2328\nEpoch 00008 | Loss 1.7506 | Val.Loss 1.8507 | Time(s) 0.2230\nEpoch 00009 | Loss 1.6602 | Val.Loss 1.8394 | Time(s) 0.2222\nEpoch 00010 | Loss 1.6732 | Val.Loss 1.8282 | Time(s) 0.2184\nEpoch 00011 | Loss 1.6241 | Val.Loss 1.8171 | Time(s) 0.2379\nEpoch 00012 | Loss 1.5776 | Val.Loss 1.8062 | Time(s) 0.2328\nEpoch 00013 | Loss 1.6381 | Val.Loss 1.7955 | Time(s) 0.2302\nEpoch 00014 | Loss 1.5291 | Val.Loss 1.7854 | Time(s) 0.2275\nEpoch 00015 | Loss 1.5804 | Val.Loss 1.7753 | Time(s) 0.2398\nEpoch 00016 | Loss 1.5226 | Val.Loss 1.7652 | Time(s) 0.2396\nEpoch 00017 | Loss 1.4970 | Val.Loss 1.7550 | Time(s) 0.2375\nEpoch 00018 | Loss 1.5074 | Val.Loss 1.7448 | Time(s) 0.2384\nEpoch 00019 | Loss 1.4835 | Val.Loss 1.7347 | Time(s) 0.2480\nEpoch 00020 | Loss 1.4821 | Val.Loss 1.7245 | Time(s) 0.2437\nEpoch 00021 | Loss 1.4264 | Val.Loss 1.7146 | Time(s) 0.2406\nEpoch 00022 | Loss 1.4395 | Val.Loss 1.7047 | Time(s) 0.2408\nEpoch 00023 | Loss 1.3880 | Val.Loss 1.6951 | Time(s) 0.2389\nEpoch 00024 | Loss 1.3731 | Val.Loss 1.6856 | Time(s) 0.2443\nEpoch 00025 | Loss 1.3639 | Val.Loss 1.6761 | Time(s) 0.2419\nEpoch 00026 | Loss 1.3514 | Val.Loss 1.6666 | Time(s) 0.2400\nEpoch 00027 | Loss 1.3356 | Val.Loss 1.6572 | Time(s) 0.2376\nEpoch 00028 | Loss 1.3338 | Val.Loss 1.6480 | Time(s) 0.2354\nEpoch 00029 | Loss 1.3205 | Val.Loss 1.6390 | Time(s) 0.2424\nEpoch 00030 | Loss 1.3157 | Val.Loss 1.6303 | Time(s) 0.2417\nEpoch 00031 | Loss 1.2798 | Val.Loss 1.6218 | Time(s) 0.2398\nEpoch 00032 | Loss 1.3649 | Val.Loss 1.6135 | Time(s) 0.2381\nEpoch 00033 | Loss 1.3083 | Val.Loss 1.6057 | Time(s) 0.2360\nEpoch 00034 | Loss 1.2578 | Val.Loss 1.5982 | Time(s) 0.2343\nEpoch 00035 | Loss 1.2394 | Val.Loss 1.5909 | Time(s) 0.2341\nEpoch 00036 | Loss 1.2567 | Val.Loss 1.5836 | Time(s) 0.2333\nEpoch 00037 | Loss 1.2128 | Val.Loss 1.5764 | Time(s) 0.2368\nEpoch 00038 | Loss 1.2039 | Val.Loss 1.5693 | Time(s) 0.2357\nEpoch 00039 | Loss 1.1931 | Val.Loss 1.5624 | Time(s) 0.2369\nEpoch 00040 | Loss 1.2094 | Val.Loss 1.5557 | Time(s) 0.2361\nEpoch 00041 | Loss 1.1654 | Val.Loss 1.5493 | Time(s) 0.2346\nEpoch 00042 | Loss 1.1611 | Val.Loss 1.5430 | Time(s) 0.2328\nEpoch 00043 | Loss 1.1854 | Val.Loss 1.5367 | Time(s) 0.2312\nEpoch 00044 | Loss 1.1299 | Val.Loss 1.5306 | Time(s) 0.2298\nEpoch 00045 | Loss 1.1141 | Val.Loss 1.5245 | Time(s) 0.2287\nEpoch 00046 | Loss 1.1044 | Val.Loss 1.5185 | Time(s) 0.2279\nEpoch 00047 | Loss 1.1120 | Val.Loss 1.5126 | Time(s) 0.2268\nEpoch 00048 | Loss 1.1253 | Val.Loss 1.5067 | Time(s) 0.2257\nEpoch 00049 | Loss 1.0644 | Val.Loss 1.5012 | Time(s) 0.2248\nEpoch 00050 | Loss 1.0531 | Val.Loss 1.4957 | Time(s) 0.2288\nEpoch 00051 | Loss 1.0505 | Val.Loss 1.4903 | Time(s) 0.2296\nEpoch 00052 | Loss 1.0501 | Val.Loss 1.4849 | Time(s) 0.2289\nEpoch 00053 | Loss 1.0663 | Val.Loss 1.4796 | Time(s) 0.2279\nEpoch 00054 | Loss 1.0645 | Val.Loss 1.4746 | Time(s) 0.2300\nEpoch 00055 | Loss 1.0097 | Val.Loss 1.4695 | Time(s) 0.2294\nEpoch 00056 | Loss 1.0139 | Val.Loss 1.4646 | Time(s) 0.2290\nEpoch 00057 | Loss 0.9773 | Val.Loss 1.4597 | Time(s) 0.2284\nEpoch 00058 | Loss 1.0771 | Val.Loss 1.4549 | Time(s) 0.2276\nEpoch 00059 | Loss 0.9660 | Val.Loss 1.4505 | Time(s) 0.2265\nEpoch 00060 | Loss 0.9864 | Val.Loss 1.4459 | Time(s) 0.2286\nEpoch 00061 | Loss 0.9368 | Val.Loss 1.4415 | Time(s) 0.2284\nEpoch 00062 | Loss 0.9479 | Val.Loss 1.4370 | Time(s) 0.2278\nEpoch 00063 | Loss 0.9622 | Val.Loss 1.4326 | Time(s) 0.2269\nEpoch 00064 | Loss 0.9298 | Val.Loss 1.4283 | Time(s) 0.2265\nEpoch 00065 | Loss 0.9201 | Val.Loss 1.4240 | Time(s) 0.2255\nEpoch 00066 | Loss 0.8909 | Val.Loss 1.4196 | Time(s) 0.2252\nEpoch 00067 | Loss 0.9040 | Val.Loss 1.4153 | Time(s) 0.2246\nEpoch 00068 | Loss 0.8943 | Val.Loss 1.4111 | Time(s) 0.2240\nEpoch 00069 | Loss 0.8752 | Val.Loss 1.4070 | Time(s) 0.2234\nEpoch 00070 | Loss 0.8665 | Val.Loss 1.4030 | Time(s) 0.2252\nEpoch 00071 | Loss 0.8509 | Val.Loss 1.3991 | Time(s) 0.2249\nEpoch 00072 | Loss 0.8524 | Val.Loss 1.3953 | Time(s) 0.2243\nEpoch 00073 | Loss 0.8404 | Val.Loss 1.3916 | Time(s) 0.2237\nEpoch 00074 | Loss 0.8331 | Val.Loss 1.3880 | Time(s) 0.2229\nEpoch 00075 | Loss 1.0110 | Val.Loss 1.3845 | Time(s) 0.2224\nEpoch 00076 | Loss 0.8315 | Val.Loss 1.3809 | Time(s) 0.2215\nEpoch 00077 | Loss 0.8248 | Val.Loss 1.3775 | Time(s) 0.2210\nEpoch 00078 | Loss 0.7823 | Val.Loss 1.3740 | Time(s) 0.2204\nEpoch 00079 | Loss 0.7766 | Val.Loss 1.3706 | Time(s) 0.2203\nEpoch 00080 | Loss 0.8556 | Val.Loss 1.3672 | Time(s) 0.2202\nEpoch 00081 | Loss 0.7663 | Val.Loss 1.3634 | Time(s) 0.2196\nEpoch 00082 | Loss 0.7511 | Val.Loss 1.3597 | Time(s) 0.2199\nEpoch 00083 | Loss 0.7449 | Val.Loss 1.3561 | Time(s) 0.2214\nEpoch 00084 | Loss 0.7389 | Val.Loss 1.3525 | Time(s) 0.2209\nEpoch 00085 | Loss 0.7368 | Val.Loss 1.3490 | Time(s) 0.2206\nEpoch 00086 | Loss 0.7482 | Val.Loss 1.3458 | Time(s) 0.2202\nEpoch 00087 | Loss 0.7336 | Val.Loss 1.3426 | Time(s) 0.2198\nEpoch 00088 | Loss 0.7227 | Val.Loss 1.3394 | Time(s) 0.2194\nEpoch 00089 | Loss 0.7210 | Val.Loss 1.3362 | Time(s) 0.2190\n"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "net = Net()\n",
    "#print(net)\n",
    "\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=1e-3)\n",
    "net.train() # Set to training mode (use dropout)\n",
    "\n",
    "dur = []\n",
    "for epoch in range(90):\n",
    "    if epoch >=3:\n",
    "        t0 = time.time()\n",
    "\n",
    "    # Compute loss for test nodes (only for validation, not used by optimizer)\n",
    "    net.eval()\n",
    "    prediction = net(g, features)\n",
    "    val_loss = F.nll_loss(prediction.detach()[1-mask], labels[1-mask])\n",
    "    net.train()\n",
    "\n",
    "    # Compute loss for train nodes\n",
    "    logits = net(g, features)\n",
    "    loss = F.nll_loss(logits[mask], labels[mask])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >=3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Val.Loss {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), val_loss.item(), np.mean(dur)))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nNLL-Loss:\n All : 1.3201 | Train: 0.6815 | Test: 1.3331 |\n\nAccuracy:\n All : 0.5768 | Train: 1.0000 | Test: 0.5682 |\n"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score as acc\n",
    "net.eval() # Set net to evaluation mode (deactivates dropout)\n",
    "final_prediction = net(g, features).detach()\n",
    "\n",
    "pred_sets = {\"All \":final_prediction,\"Train\":final_prediction[mask],\"Test\":final_prediction[1-mask]}\n",
    "label_sets = {\"All \":labels,\"Train\":labels[mask],\"Test\":labels[1-mask]}\n",
    "eval_functions = {\"NLL-Loss\":lambda y,x: F.nll_loss(x,y),\"Accuracy\":lambda y,x: acc(y,x.numpy().argmax(axis=1))}\n",
    "\n",
    "for name,func in eval_functions.items():\n",
    "    eval_message = f\"\\n{name}:\\n\"\n",
    "    for subset in pred_sets.keys():\n",
    "        eval_message += f\" {subset}: {func(label_sets[subset],pred_sets[subset]):.4f} |\"\n",
    "    print(eval_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}