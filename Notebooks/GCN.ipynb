{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Convolutional Network by Kipf and Welling"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GNN Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModule(nn.Module):\n",
    "    \"\"\"The linear transformation part of the GCN layer\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(LinearModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation # This is the activation function\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        h = self.activation(h)\n",
    "        return {'h' : h}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\"A GCN layer\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = LinearModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(message_func=fn.copy_src(src='h', out='m'), reduce_func=fn.sum(msg='m', out='h'))\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.gcn1 = GCN(1433, 21, F.relu)\n",
    "        self.gcn2 = GCN(21, 7, lambda x: F.log_softmax(x,1))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = self.gcn1(g, features)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.gcn2(g, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import citation_graph as citegrh\n",
    "import networkx as nx\n",
    "\n",
    "data = citegrh.load_cora()\n",
    "features = th.FloatTensor(data.features)\n",
    "labels = th.LongTensor(data.labels)\n",
    "mask = th.ByteTensor(data.train_mask)\n",
    "g = data.graph\n",
    "\n",
    "# add self loop\n",
    "g.remove_edges_from(nx.selfloop_edges(g))\n",
    "g = DGLGraph(g)\n",
    "g.add_edges(g.nodes(), g.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_train = 0.02\n",
    "\n",
    "with open(\"data/cora_permutation1.pickle\",\"rb\") as f:\n",
    "    perm1 = pickle.load(f)\n",
    "mask = np.zeros(g.number_of_nodes())\n",
    "mask[perm1[range(int(percentage_train*g.number_of_nodes()))]] = 1\n",
    "mask = th.ByteTensor(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_perms = list(itertools.permutations(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Users\\a_liso02\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n  out=out, **kwargs)\nC:\\Users\\a_liso02\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\nEpoch 00000 | Loss 1.8830 | Val.Loss 1.2637 | Time(s) nan\nEpoch 00001 | Loss 1.8500 | Val.Loss 1.2637 | Time(s) nan\nEpoch 00002 | Loss 1.8179 | Val.Loss 1.2637 | Time(s) nan\nEpoch 00003 | Loss 1.7869 | Val.Loss 1.2637 | Time(s) 0.9800\nEpoch 00004 | Loss 1.7514 | Val.Loss 1.2637 | Time(s) 1.0095\nEpoch 00005 | Loss 1.7163 | Val.Loss 1.2637 | Time(s) 0.9943\nEpoch 00006 | Loss 1.6809 | Val.Loss 1.2637 | Time(s) 0.9865\nEpoch 00007 | Loss 1.6463 | Val.Loss 1.2637 | Time(s) 0.9942\nEpoch 00008 | Loss 1.6134 | Val.Loss 1.2637 | Time(s) 0.9912\nEpoch 00009 | Loss 1.5825 | Val.Loss 1.2637 | Time(s) 0.9889\nEpoch 00010 | Loss 1.5533 | Val.Loss 1.2637 | Time(s) 0.9881\nEpoch 00011 | Loss 1.5259 | Val.Loss 1.2637 | Time(s) 0.9863\nEpoch 00012 | Loss 1.5003 | Val.Loss 1.2637 | Time(s) 0.9856\nEpoch 00013 | Loss 1.4764 | Val.Loss 1.2637 | Time(s) 0.9846\nEpoch 00014 | Loss 1.4538 | Val.Loss 1.2637 | Time(s) 0.9830\nEpoch 00015 | Loss 1.4323 | Val.Loss 1.2637 | Time(s) 0.9849\nEpoch 00016 | Loss 1.4118 | Val.Loss 1.2637 | Time(s) 0.9839\nEpoch 00017 | Loss 1.3922 | Val.Loss 1.2637 | Time(s) 0.9835\nEpoch 00018 | Loss 1.3736 | Val.Loss 1.2637 | Time(s) 0.9828\nEpoch 00019 | Loss 1.3558 | Val.Loss 1.2637 | Time(s) 0.9823\nEpoch 00020 | Loss 1.3386 | Val.Loss 1.2637 | Time(s) 0.9835\nEpoch 00021 | Loss 1.3218 | Val.Loss 1.2637 | Time(s) 0.9829\nEpoch 00022 | Loss 1.3055 | Val.Loss 1.2637 | Time(s) 0.9837\nEpoch 00023 | Loss 1.2895 | Val.Loss 1.2637 | Time(s) 0.9842\nEpoch 00024 | Loss 1.2737 | Val.Loss 1.2637 | Time(s) 0.9840\nEpoch 00025 | Loss 1.2583 | Val.Loss 1.2637 | Time(s) 0.9842\nEpoch 00026 | Loss 1.2430 | Val.Loss 1.2637 | Time(s) 0.9843\nEpoch 00027 | Loss 1.2281 | Val.Loss 1.2637 | Time(s) 0.9844\nEpoch 00028 | Loss 1.2136 | Val.Loss 1.2637 | Time(s) 0.9857\nEpoch 00029 | Loss 1.1993 | Val.Loss 1.2637 | Time(s) 0.9872\nEpoch 00030 | Loss 1.1853 | Val.Loss 1.2637 | Time(s) 0.9869\nEpoch 00031 | Loss 1.1714 | Val.Loss 1.2637 | Time(s) 0.9864\nEpoch 00032 | Loss 1.1577 | Val.Loss 1.2637 | Time(s) 0.9860\nEpoch 00033 | Loss 1.1441 | Val.Loss 1.2637 | Time(s) 0.9855\nEpoch 00034 | Loss 1.1306 | Val.Loss 1.2637 | Time(s) 0.9848\nEpoch 00035 | Loss 1.1173 | Val.Loss 1.2637 | Time(s) 0.9844\nEpoch 00036 | Loss 1.1043 | Val.Loss 1.2637 | Time(s) 0.9842\nEpoch 00037 | Loss 1.0913 | Val.Loss 1.2637 | Time(s) 0.9836\nEpoch 00038 | Loss 1.0785 | Val.Loss 1.2637 | Time(s) 0.9837\nEpoch 00039 | Loss 1.0658 | Val.Loss 1.2637 | Time(s) 0.9831\nEpoch 00040 | Loss 1.0532 | Val.Loss 1.2637 | Time(s) 0.9826\nEpoch 00041 | Loss 1.0406 | Val.Loss 1.2637 | Time(s) 0.9829\nEpoch 00042 | Loss 1.0280 | Val.Loss 1.2637 | Time(s) 0.9830\nEpoch 00043 | Loss 1.0156 | Val.Loss 1.2637 | Time(s) 0.9828\nEpoch 00044 | Loss 1.0032 | Val.Loss 1.2637 | Time(s) 0.9832\nEpoch 00045 | Loss 0.9908 | Val.Loss 1.2637 | Time(s) 0.9829\nEpoch 00046 | Loss 0.9785 | Val.Loss 1.2637 | Time(s) 0.9826\nEpoch 00047 | Loss 0.9663 | Val.Loss 1.2637 | Time(s) 0.9824\nEpoch 00048 | Loss 0.9541 | Val.Loss 1.2637 | Time(s) 0.9820\nEpoch 00049 | Loss 0.9419 | Val.Loss 1.2637 | Time(s) 0.9818\nEpoch 00050 | Loss 0.9298 | Val.Loss 1.2637 | Time(s) 0.9820\nEpoch 00051 | Loss 0.9177 | Val.Loss 1.2637 | Time(s) 0.9821\nEpoch 00052 | Loss 0.9057 | Val.Loss 1.2637 | Time(s) 0.9818\nEpoch 00053 | Loss 0.8938 | Val.Loss 1.2637 | Time(s) 0.9817\nEpoch 00054 | Loss 0.8820 | Val.Loss 1.2637 | Time(s) 0.9815\nEpoch 00055 | Loss 0.8701 | Val.Loss 1.2637 | Time(s) 0.9814\nEpoch 00056 | Loss 0.8584 | Val.Loss 1.2637 | Time(s) 0.9818\nEpoch 00057 | Loss 0.8467 | Val.Loss 1.2637 | Time(s) 0.9815\nEpoch 00058 | Loss 0.8351 | Val.Loss 1.2637 | Time(s) 0.9812\nEpoch 00059 | Loss 0.8236 | Val.Loss 1.2637 | Time(s) 0.9810\nEpoch 00060 | Loss 0.8122 | Val.Loss 1.2637 | Time(s) 0.9809\nEpoch 00061 | Loss 0.8008 | Val.Loss 1.2637 | Time(s) 0.9806\nEpoch 00062 | Loss 0.7895 | Val.Loss 1.2637 | Time(s) 0.9805\nEpoch 00063 | Loss 0.7783 | Val.Loss 1.2637 | Time(s) 0.9803\nEpoch 00064 | Loss 0.7672 | Val.Loss 1.2637 | Time(s) 0.9802\nEpoch 00065 | Loss 0.7561 | Val.Loss 1.2637 | Time(s) 0.9803\nEpoch 00066 | Loss 0.7451 | Val.Loss 1.2637 | Time(s) 0.9805\nEpoch 00067 | Loss 0.7342 | Val.Loss 1.2637 | Time(s) 0.9821\nEpoch 00068 | Loss 0.7234 | Val.Loss 1.2637 | Time(s) 0.9821\nEpoch 00069 | Loss 0.7127 | Val.Loss 1.2637 | Time(s) 0.9821\nEpoch 00070 | Loss 0.7020 | Val.Loss 1.2637 | Time(s) 0.9819\nEpoch 00071 | Loss 0.6915 | Val.Loss 1.2637 | Time(s) 0.9818\nEpoch 00072 | Loss 0.6810 | Val.Loss 1.2637 | Time(s) 0.9820\nEpoch 00073 | Loss 0.6707 | Val.Loss 1.2637 | Time(s) 0.9824\nEpoch 00074 | Loss 0.6605 | Val.Loss 1.2637 | Time(s) 0.9828\nEpoch 00075 | Loss 0.6503 | Val.Loss 1.2637 | Time(s) 0.9828\nEpoch 00076 | Loss 0.6403 | Val.Loss 1.2637 | Time(s) 0.9829\nEpoch 00077 | Loss 0.6304 | Val.Loss 1.2637 | Time(s) 0.9832\nEpoch 00078 | Loss 0.6205 | Val.Loss 1.2637 | Time(s) 0.9832\nEpoch 00079 | Loss 0.6108 | Val.Loss 1.2637 | Time(s) 0.9836\nEpoch 00080 | Loss 0.6012 | Val.Loss 1.2637 | Time(s) 0.9837\nEpoch 00081 | Loss 0.5917 | Val.Loss 1.2637 | Time(s) 0.9848\nEpoch 00082 | Loss 0.5822 | Val.Loss 1.2637 | Time(s) 0.9850\nEpoch 00083 | Loss 0.5729 | Val.Loss 1.2637 | Time(s) 0.9856\nEpoch 00084 | Loss 0.5637 | Val.Loss 1.2637 | Time(s) 0.9857\nEpoch 00085 | Loss 0.5546 | Val.Loss 1.2637 | Time(s) 0.9859\nEpoch 00086 | Loss 0.5456 | Val.Loss 1.2637 | Time(s) 0.9858\nEpoch 00087 | Loss 0.5367 | Val.Loss 1.2637 | Time(s) 0.9858\nEpoch 00088 | Loss 0.5280 | Val.Loss 1.2637 | Time(s) 0.9857\nEpoch 00089 | Loss 0.5193 | Val.Loss 1.2637 | Time(s) 0.9858\nEpoch 00090 | Loss 0.5108 | Val.Loss 1.2637 | Time(s) 0.9857\nEpoch 00091 | Loss 0.5023 | Val.Loss 1.2637 | Time(s) 0.9857\nEpoch 00092 | Loss 0.4940 | Val.Loss 1.2637 | Time(s) 0.9856\nEpoch 00093 | Loss 0.4858 | Val.Loss 1.2637 | Time(s) 0.9854\nEpoch 00094 | Loss 0.4777 | Val.Loss 1.2637 | Time(s) 0.9853\nEpoch 00095 | Loss 0.4697 | Val.Loss 1.2637 | Time(s) 0.9854\nEpoch 00096 | Loss 0.4618 | Val.Loss 1.2637 | Time(s) 0.9852\nEpoch 00097 | Loss 0.4541 | Val.Loss 1.2637 | Time(s) 0.9851\nEpoch 00098 | Loss 0.4464 | Val.Loss 1.2637 | Time(s) 0.9850\nEpoch 00099 | Loss 0.4388 | Val.Loss 1.2637 | Time(s) 0.9850\nEpoch 00100 | Loss 0.4314 | Val.Loss 1.2637 | Time(s) 0.9849\nEpoch 00101 | Loss 0.4240 | Val.Loss 1.2637 | Time(s) 0.9847\nEpoch 00102 | Loss 0.4168 | Val.Loss 1.2637 | Time(s) 0.9846\nEpoch 00103 | Loss 0.4097 | Val.Loss 1.2637 | Time(s) 0.9847\nEpoch 00104 | Loss 0.4027 | Val.Loss 1.2637 | Time(s) 0.9847\nEpoch 00105 | Loss 0.3957 | Val.Loss 1.2637 | Time(s) 0.9847\nEpoch 00106 | Loss 0.3890 | Val.Loss 1.2637 | Time(s) 0.9846\nEpoch 00107 | Loss 0.3823 | Val.Loss 1.2637 | Time(s) 0.9845\nEpoch 00108 | Loss 0.3757 | Val.Loss 1.2637 | Time(s) 0.9844\nEpoch 00109 | Loss 0.3692 | Val.Loss 1.2637 | Time(s) 0.9844\nEpoch 00110 | Loss 0.3629 | Val.Loss 1.2637 | Time(s) 0.9843\nEpoch 00111 | Loss 0.3566 | Val.Loss 1.2637 | Time(s) 0.9843\nEpoch 00112 | Loss 0.3505 | Val.Loss 1.2637 | Time(s) 0.9843\nEpoch 00113 | Loss 0.3444 | Val.Loss 1.2637 | Time(s) 0.9841\nEpoch 00114 | Loss 0.3385 | Val.Loss 1.2637 | Time(s) 0.9842\nEpoch 00115 | Loss 0.3326 | Val.Loss 1.2637 | Time(s) 0.9840\nEpoch 00116 | Loss 0.3269 | Val.Loss 1.2637 | Time(s) 0.9840\nEpoch 00117 | Loss 0.3213 | Val.Loss 1.2637 | Time(s) 0.9839\nEpoch 00118 | Loss 0.3157 | Val.Loss 1.2637 | Time(s) 0.9838\nEpoch 00119 | Loss 0.3103 | Val.Loss 1.2637 | Time(s) 0.9839\nEpoch 00120 | Loss 0.3050 | Val.Loss 1.2637 | Time(s) 0.9839\nEpoch 00121 | Loss 0.2997 | Val.Loss 1.2637 | Time(s) 0.9838\nEpoch 00122 | Loss 0.2946 | Val.Loss 1.2637 | Time(s) 0.9837\nEpoch 00123 | Loss 0.2895 | Val.Loss 1.2637 | Time(s) 0.9837\nEpoch 00124 | Loss 0.2846 | Val.Loss 1.2637 | Time(s) 0.9837\nEpoch 00125 | Loss 0.2797 | Val.Loss 1.2637 | Time(s) 0.9835\nEpoch 00126 | Loss 0.2749 | Val.Loss 1.2637 | Time(s) 0.9834\nEpoch 00127 | Loss 0.2703 | Val.Loss 1.2637 | Time(s) 0.9837\nEpoch 00128 | Loss 0.2657 | Val.Loss 1.2637 | Time(s) 0.9838\nEpoch 00129 | Loss 0.2612 | Val.Loss 1.2637 | Time(s) 0.9838\nEpoch 00130 | Loss 0.2568 | Val.Loss 1.2637 | Time(s) 0.9837\nEpoch 00131 | Loss 0.2525 | Val.Loss 1.2637 | Time(s) 0.9838\nEpoch 00132 | Loss 0.2482 | Val.Loss 1.2637 | Time(s) 0.9837\nEpoch 00133 | Loss 0.2440 | Val.Loss 1.2637 | Time(s) 0.9836\nEpoch 00134 | Loss 0.2400 | Val.Loss 1.2637 | Time(s) 0.9836\nEpoch 00135 | Loss 0.2360 | Val.Loss 1.2637 | Time(s) 0.9837\nEpoch 00136 | Loss 0.2320 | Val.Loss 1.2637 | Time(s) 0.9835\nEpoch 00137 | Loss 0.2282 | Val.Loss 1.2637 | Time(s) 0.9835\nEpoch 00138 | Loss 0.2244 | Val.Loss 1.2637 | Time(s) 0.9834\nEpoch 00139 | Loss 0.2207 | Val.Loss 1.2637 | Time(s) 0.9833\nEpoch 00140 | Loss 0.2171 | Val.Loss 1.2637 | Time(s) 0.9836\nEpoch 00141 | Loss 0.2135 | Val.Loss 1.2637 | Time(s) 0.9836\nEpoch 00142 | Loss 0.2100 | Val.Loss 1.2637 | Time(s) 0.9835\nEpoch 00143 | Loss 0.2066 | Val.Loss 1.2637 | Time(s) 0.9836\nEpoch 00144 | Loss 0.2032 | Val.Loss 1.2637 | Time(s) 0.9836\nEpoch 00145 | Loss 0.1999 | Val.Loss 1.2637 | Time(s) 0.9842\nEpoch 00146 | Loss 0.1967 | Val.Loss 1.2637 | Time(s) 0.9841\nEpoch 00147 | Loss 0.1936 | Val.Loss 1.2637 | Time(s) 0.9841\nEpoch 00148 | Loss 0.1905 | Val.Loss 1.2637 | Time(s) 0.9840\nEpoch 00149 | Loss 0.1874 | Val.Loss 1.2637 | Time(s) 0.9843\n"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "net = Net()\n",
    "#print(net)\n",
    "\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=1e-3)\n",
    "net.train() # Set to training mode (use dropout)\n",
    "\n",
    "dur = []\n",
    "for epoch in range(150):\n",
    "    if epoch >=3:\n",
    "        t0 = time.time()\n",
    "\n",
    "    # Compute loss for test nodes (only for validation, not used by optimizer)\n",
    "    net.eval()\n",
    "    prediction = net(g, features)\n",
    "    #scores = th.tensor([F.nll_loss(prediction.detach()[1-mask][:,p], labels[1-mask]) for p in label_perms])\n",
    "    #val_loss = th.min(scores)\n",
    "    #val_loss = F.nll_loss(prediction.detach()[1-mask], labels[1-mask])\n",
    "    net.train()\n",
    "\n",
    "    # Compute loss for train nodes\n",
    "    logits = net(g, features)\n",
    "\n",
    "    loss = th.tensor(1000.0,requires_grad=True)\n",
    "    for p in label_perms:\n",
    "        loss = th.min(loss,F.nll_loss(logits[mask][:,p], labels[mask]))\n",
    "\n",
    "    #loss = F.nll_loss(logits[mask], labels[mask])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >=3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Val.Loss {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), val_loss.item(), np.mean(dur)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0],\n       [0, 0],\n       [0, 0],\n       [0, 0],\n       [0, 0],\n       [1, 2],\n       [1, 2],\n       [1, 2],\n       [1, 2],\n       [1, 2],\n       [1, 2],\n       [1, 2],\n       [1, 2],\n       [1, 2],\n       [1, 2],\n       [1, 2],\n       [1, 2],\n       [1, 2],\n       [2, 6],\n       [2, 6],\n       [2, 6],\n       [2, 6],\n       [2, 6],\n       [2, 6],\n       [2, 6],\n       [3, 4],\n       [3, 4],\n       [3, 4],\n       [3, 4],\n       [3, 4],\n       [3, 4],\n       [3, 4],\n       [4, 3],\n       [4, 3],\n       [4, 3],\n       [4, 3],\n       [4, 3],\n       [4, 3],\n       [4, 3],\n       [4, 3],\n       [4, 3],\n       [4, 3],\n       [4, 3],\n       [5, 5],\n       [5, 5],\n       [6, 1],\n       [6, 1],\n       [6, 1],\n       [6, 1],\n       [6, 1],\n       [6, 1],\n       [6, 1],\n       [6, 1],\n       [6, 1]], dtype=int64)"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualise predictions\n",
    "net.eval() # Set net to evaluation mode (deactivates dropout)\n",
    "final_prediction = net(g, features).detach()\n",
    "a = np.transpose(np.vstack([final_prediction[mask].numpy().argmax(axis=1),labels[mask].numpy()]))\n",
    "a[a[:,0].argsort()]\n",
    "# as can be seen, the net predicts other labels, but gets the clusters right :)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nNLL-Loss:\n All : 1.2504 | Train: 0.1874 | Test: 1.2720 |\n\nAccuracy:\n All : 0.5934 | Train: 1.0000 | Test: 0.5852 |\n"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score as acc\n",
    "net.eval() # Set net to evaluation mode (deactivates dropout)\n",
    "final_prediction = net(g, features).detach()\n",
    "\n",
    "pred_sets = {\"All \":final_prediction,\"Train\":final_prediction[mask],\"Test\":final_prediction[1-mask]}\n",
    "label_sets = {\"All \":labels,\"Train\":labels[mask],\"Test\":labels[1-mask]}\n",
    "eval_functions = {\"NLL-Loss\":lambda y,x: F.nll_loss(x,y),\"Accuracy\":lambda y,x: acc(y,x.numpy().argmax(axis=1))}\n",
    "\n",
    "for name,func in eval_functions.items():\n",
    "    eval_message = f\"\\n{name}:\\n\"\n",
    "    for subset in pred_sets.keys():\n",
    "        eval_message += f\" {subset}: {func(label_sets[subset],pred_sets[subset]):.4f} |\"\n",
    "    print(eval_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}