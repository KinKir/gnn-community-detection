{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Attention Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "\n",
    "import performance as pf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, g, in_dim, out_dim):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.g = g\n",
    "        # equation (1)\n",
    "        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        # equation (2)\n",
    "        self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        # edge UDF for equation (2)\n",
    "        z2 = th.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
    "        a = self.attn_fc(z2)\n",
    "        return {'e': F.leaky_relu(a)}\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        # message UDF for equation (3) & (4)\n",
    "        return {'z': edges.src['z'], 'e': edges.data['e']}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        # reduce UDF for equation (3) & (4)\n",
    "        # equation (3)\n",
    "        alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
    "        # equation (4)\n",
    "        h = th.sum(alpha * nodes.mailbox['z'], dim=1)\n",
    "        return {'h': h}\n",
    "\n",
    "    def forward(self, h):\n",
    "        # equation (1)\n",
    "        z = self.fc(h)\n",
    "        self.g.ndata['z'] = z\n",
    "        # equation (2)\n",
    "        self.g.apply_edges(self.edge_attention)\n",
    "        # equation (3) & (4)\n",
    "        self.g.update_all(self.message_func, self.reduce_func)\n",
    "        return self.g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGATLayer(nn.Module):\n",
    "    def __init__(self, g, in_dim, out_dim, num_heads, merge='cat'):\n",
    "        super(MultiHeadGATLayer, self).__init__()\n",
    "        self.heads = nn.ModuleList()\n",
    "        for i in range(num_heads):\n",
    "            self.heads.append(GATLayer(g, in_dim, out_dim))\n",
    "        self.merge = merge\n",
    "\n",
    "    def forward(self, h):\n",
    "        head_outs = [attn_head(h) for attn_head in self.heads]\n",
    "        if self.merge == 'cat':\n",
    "            # concat on the output feature dimension (dim=1)\n",
    "            return th.cat(head_outs, dim=1)\n",
    "        else:\n",
    "            # merge using average\n",
    "            return th.mean(torch.stack(head_outs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, g, in_dim, hidden_dim, out_dim, num_heads):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = MultiHeadGATLayer(g, in_dim, hidden_dim, num_heads)\n",
    "        # Be aware that the input dimension is hidden_dim*num_heads since\n",
    "        # multiple head outputs are concatenated together. Also, only\n",
    "        # one attention head in the output layer.\n",
    "        self.layer2 = MultiHeadGATLayer(g, hidden_dim * num_heads, out_dim, 1)\n",
    "\n",
    "    def forward(self, h):\n",
    "        h = self.layer1(h)\n",
    "        h = F.elu(h)\n",
    "        h = self.layer2(h)\n",
    "        h = F.log_softmax(h, 1)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dgl.pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-43f09c91e765>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGAT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dgl.pytorch'"
     ]
    }
   ],
   "source": [
    "from dgl.pytorch.gat import GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'GAT' from 'C:\\\\Users\\\\User\\\\Documents\\\\dev\\\\gnn-community-detection\\\\Notebooks\\\\GAT.py'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(GAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import citation_graph as citegrh\n",
    "import networkx as nx\n",
    "\n",
    "data = citegrh.load_cora()\n",
    "features = th.FloatTensor(data.features)\n",
    "labels = th.LongTensor(data.labels)\n",
    "mask = th.ByteTensor(data.train_mask)\n",
    "g = data.graph\n",
    "\n",
    "# add self loop\n",
    "g.remove_edges_from(nx.selfloop_edges(g))\n",
    "g = DGLGraph(g)\n",
    "g.add_edges(g.nodes(), g.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_train = 0.1\n",
    "percentage_val = 0.1\n",
    "\n",
    "with open(\"../data/permutations/cora_permutation1.pickle\",\"rb\") as f:\n",
    "    perm1 = pickle.load(f)\n",
    "mask_train = np.zeros(g.number_of_nodes())\n",
    "mask_val = np.zeros(g.number_of_nodes())\n",
    "\n",
    "i_train = int(percentage_train*g.number_of_nodes())\n",
    "i_val = i_train + int(percentage_val*g.number_of_nodes())\n",
    "mask_train[perm1[range(0,i_train)]] = 1\n",
    "mask_val[perm1[range(i_train,i_val)]] = 1\n",
    "mask_train = th.BoolTensor(mask_train)\n",
    "mask_val = th.BoolTensor(mask_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = pf.perm_inv_loss(labels)\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 3.6102 | Train.Rand 0.0487 | Valid.Rand 0.0394 | Time(s) unknown\n",
      "Epoch 00001 | Loss 1.2399 | Train.Rand 0.5547 | Valid.Rand 0.4095 | Time(s) unknown\n",
      "Epoch 00002 | Loss 0.7482 | Train.Rand 0.7176 | Valid.Rand 0.5228 | Time(s) unknown\n",
      "Epoch 00003 | Loss 0.6428 | Train.Rand 0.7684 | Valid.Rand 0.5690 | Time(s) 0.4239\n",
      "Epoch 00004 | Loss 0.5783 | Train.Rand 0.8108 | Valid.Rand 0.5673 | Time(s) 0.4171\n",
      "Epoch 00005 | Loss 0.5111 | Train.Rand 0.8571 | Valid.Rand 0.5918 | Time(s) 0.4395\n",
      "Epoch 00006 | Loss 0.4231 | Train.Rand 0.8879 | Valid.Rand 0.5674 | Time(s) 0.4422\n",
      "Epoch 00007 | Loss 0.4471 | Train.Rand 0.8826 | Valid.Rand 0.5777 | Time(s) 0.4438\n",
      "Epoch 00008 | Loss 0.4225 | Train.Rand 0.9049 | Valid.Rand 0.5585 | Time(s) 0.4401\n",
      "Epoch 00009 | Loss 0.4536 | Train.Rand 0.9229 | Valid.Rand 0.5829 | Time(s) 0.4375\n",
      "Epoch 00010 | Loss 0.2907 | Train.Rand 0.9169 | Valid.Rand 0.5438 | Time(s) 0.4348\n",
      "Epoch 00011 | Loss 0.3701 | Train.Rand 0.9435 | Valid.Rand 0.5231 | Time(s) 0.4337\n",
      "Epoch 00012 | Loss 0.2924 | Train.Rand 0.9328 | Valid.Rand 0.5270 | Time(s) 0.4309\n",
      "Epoch 00013 | Loss 0.2338 | Train.Rand 0.9362 | Valid.Rand 0.5336 | Time(s) 0.4301\n",
      "Epoch 00014 | Loss 0.1982 | Train.Rand 0.9429 | Valid.Rand 0.5520 | Time(s) 0.4295\n",
      "Epoch 00015 | Loss 0.2020 | Train.Rand 0.9647 | Valid.Rand 0.5482 | Time(s) 0.4331\n",
      "Epoch 00016 | Loss 0.2309 | Train.Rand 0.9610 | Valid.Rand 0.5742 | Time(s) 0.4356\n",
      "Epoch 00017 | Loss 0.2258 | Train.Rand 0.9676 | Valid.Rand 0.5783 | Time(s) 0.4354\n",
      "Epoch 00018 | Loss 0.1257 | Train.Rand 0.9618 | Valid.Rand 0.5987 | Time(s) 0.4356\n",
      "Epoch 00019 | Loss 0.2609 | Train.Rand 0.9711 | Valid.Rand 0.5952 | Time(s) 0.4348\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-75370fa47b1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "net = GAT.GAT_Net_fast(g=g, in_feats=features.shape[1], hidden_size=100, hidden_layers=2, out_feats=len(np.unique(labels)),\n",
    "dropout=0.2, batchnorm=False, num_heads=1, residual = False)\n",
    "#print(net)\n",
    "\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=1e-2, weight_decay=1e-2)\n",
    "net.train() # Set to training mode (use dropout)\n",
    "\n",
    "dur = []\n",
    "loss_ev = []\n",
    "current_best = 0 #arbitrarily high\n",
    "current_best_epoch = 0\n",
    "current_best_params = None\n",
    "no_improvement_for = 0\n",
    "\n",
    "for epoch in range(10000):\n",
    "    if epoch >=3:\n",
    "        t0 = time.time()\n",
    "\n",
    "    # Compute loss for test nodes (only for validation, not used by optimizer)\n",
    "    net.eval()\n",
    "    prediction = net(features)\n",
    "    train_rand=pf.rand_score(labels[mask_train].numpy(),np.argmax(prediction[mask_train].detach().numpy(), axis=1))\n",
    "    validation_rand=pf.rand_score(labels[mask_val].numpy(),np.argmax(prediction[mask_val].detach().numpy(), axis=1))\n",
    "    if train_rand>current_best:\n",
    "        current_best = train_rand\n",
    "        current_best_epoch = epoch\n",
    "        current_best_params = copy.deepcopy(net.state_dict())\n",
    "        no_improvement_for = 0\n",
    "    else: no_improvement_for += 1\n",
    "    \n",
    "    if no_improvement_for>100:\n",
    "        break\n",
    "    \n",
    "    net.train()\n",
    "\n",
    "    # Compute loss for train nodes\n",
    "    logits = net(features)\n",
    "\n",
    "    #loss = loss_function.approximate_loss(logits,mask_train,nclasses=7)\n",
    "    loss = F.nll_loss(logits[mask_train], labels[mask_train])\n",
    "    loss_ev.append(loss.detach().item())\n",
    "    #print(np.unique(np.argmax(logits[mask_train].detach().numpy(),1)))\n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >=3:\n",
    "        dur.append(time.time() - t0)\n",
    "        print(f\"Epoch {epoch:05d} | Loss {loss.item():.4f} | Train.Rand {train_rand:.4f} | Valid.Rand {validation_rand:.4f} | Time(s) {np.mean(dur):.4f}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch:05d} | Loss {loss.item():.4f} | Train.Rand {train_rand:.4f} | Valid.Rand {validation_rand:.4f} | Time(s) unknown\")\n",
    "        \n",
    "net.load_state_dict(current_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(current_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.argmax(logits[mask_train].detach().numpy(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.argmax(np.exp(prediction.detach()).numpy(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99999994, 1.0000001 , 1.0000001 , ..., 1.        , 0.9999999 ,\n",
       "       0.9999999 ], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.exp(prediction.detach()).numpy(),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 6],\n",
       "       [0, 3],\n",
       "       [2, 2],\n",
       "       [2, 2],\n",
       "       [0, 6],\n",
       "       [6, 4],\n",
       "       [4, 0],\n",
       "       [4, 0],\n",
       "       [0, 2],\n",
       "       [0, 4]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualise predictions\n",
    "net.eval() # Set net to evaluation mode (deactivates dropout)\n",
    "final_prediction = net(g, features).detach()\n",
    "a = np.transpose(np.vstack([final_prediction[mask].numpy().argmax(axis=1),labels[mask].numpy()]))\n",
    "a[a[:,0].argsort()][np.random.choice(range(a.shape[0]),size=10)]\n",
    "# as can be seen, the net predicts other labels, but gets the clusters right :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>All</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Rand-Index</td>\n",
       "      <td>0.533541</td>\n",
       "      <td>0.576580</td>\n",
       "      <td>0.533674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mutual Information</td>\n",
       "      <td>0.535263</td>\n",
       "      <td>0.560018</td>\n",
       "      <td>0.536314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Variation of Information</td>\n",
       "      <td>1.721891</td>\n",
       "      <td>1.462670</td>\n",
       "      <td>1.717056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              All      Train      Test\n",
       "Rand-Index                0.533541  0.576580  0.533674\n",
       "Mutual Information        0.535263  0.560018  0.536314\n",
       "Variation of Information  1.721891  1.462670  1.717056"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval() # Set net to evaluation mode (deactivates dropout)\n",
    "final_prediction = net(features).detach()\n",
    "pf.performance_as_df(labels,final_prediction,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
